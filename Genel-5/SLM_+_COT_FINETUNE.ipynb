{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPiG+IsBhY9lUPrholCiKS5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emredeveloper/Transformers--General-AI/blob/main/SLM_%2B_COT_FINETUNE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "Di-Z5MYNDD6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConversationDataset(Dataset):\n",
        "    def __init__(self, dataset, tokenizer, max_length=512):\n",
        "        self.examples = []\n",
        "\n",
        "        print(\"Processing conversations...\")\n",
        "        for item in tqdm(dataset):\n",
        "            # Format conversation\n",
        "            conversation = \"\"\n",
        "            for turn in item['chosen']:\n",
        "                role = turn['role']\n",
        "                content = turn['content']\n",
        "                if role == 'user':\n",
        "                    conversation += f\"Human: {content}\\n\"\n",
        "                else:\n",
        "                    conversation += f\"Assistant: {content}\\n\"\n",
        "\n",
        "            # Tokenize\n",
        "            encodings = tokenizer(\n",
        "                conversation,\n",
        "                truncation=True,\n",
        "                max_length=max_length,\n",
        "                padding=\"max_length\",\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "\n",
        "            self.examples.append({\n",
        "                \"input_ids\": encodings[\"input_ids\"][0],\n",
        "                \"attention_mask\": encodings[\"attention_mask\"][0],\n",
        "                \"labels\": encodings[\"input_ids\"][0].clone()\n",
        "            })\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.examples[idx]"
      ],
      "metadata": {
        "id": "uLZ6RwUwcs_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Load dataset\n",
        "    print(\"Loading dataset...\")\n",
        "    dataset = load_dataset(\"kenhktsui/longtalk-cot-v0.1\")\n",
        "\n",
        "    # Kullan küçük bir subset (test için)\n",
        "    dataset['train'] = dataset['train'].select(range(1000))\n",
        "\n",
        "    # Load model and tokenizer\n",
        "    print(\"Loading model and tokenizer...\")\n",
        "    model_name = \"HuggingFaceTB/SmolLM-135M\"\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "        model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "    # Split dataset\n",
        "    print(\"Splitting dataset...\")\n",
        "    train_size = int(0.9 * len(dataset['train']))\n",
        "    val_size = len(dataset['train']) - train_size\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
        "        dataset['train'],\n",
        "        [train_size, val_size]\n",
        "    )\n",
        "\n",
        "    # Prepare datasets\n",
        "    print(\"Preparing training dataset...\")\n",
        "    train_dataset = ConversationDataset(train_dataset, tokenizer)\n",
        "    print(\"Preparing validation dataset...\")\n",
        "    eval_dataset = ConversationDataset(val_dataset, tokenizer)\n",
        "\n",
        "    # Training arguments\n",
        "    # Training arguments\n",
        "    training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=100,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=500,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=500,\n",
        "    load_best_model_at_end=True,\n",
        "    gradient_accumulation_steps=4,\n",
        "    fp16=True,\n",
        "    report_to=\"none\"\n",
        "    )\n",
        "\n",
        "    # Initialize trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset,\n",
        "        data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "    )\n",
        "\n",
        "    # Start training\n",
        "    trainer.train()\n",
        "\n",
        "    # Save model\n",
        "    model_save_path = \"./fine_tuned_smolLM\"\n",
        "    trainer.save_model(model_save_path)\n",
        "    tokenizer.save_pretrained(model_save_path)"
      ],
      "metadata": {
        "id": "4qdPDUcXdboZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "Y9a_q7mxdfLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model değerlendirme için gerekli fonksiyonlar ve test kodu\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "def model_yukle():\n",
        "    model_path = \"./fine_tuned_smolLM\"\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "    return model, tokenizer\n",
        "\n",
        "def yanit_uret(prompt, model, tokenizer, max_length=250):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model = model.to(device)\n",
        "\n",
        "    # CoT formatında prompt hazırla\n",
        "    formatted_prompt = f\"Question: {prompt}\\nLet's solve this step by step:\\n\"\n",
        "\n",
        "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            max_length=max_length,\n",
        "            num_return_sequences=1,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.pad_token_id\n",
        "        )\n",
        "\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Chain-of-Thought tarzında test soruları\n",
        "test_ornekleri = [\n",
        "    \"What is the sum of the first 5 prime numbers?\",\n",
        "    \"What is 1 + 1?\"\n",
        "]\n",
        "\n",
        "# Modeli yükle ve test et\n",
        "print(\"Model Evaluation Results:\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "model, tokenizer = model_yukle()\n",
        "\n",
        "for ornek in test_ornekleri:\n",
        "    print(f\"\\nQuestion: {ornek}\")\n",
        "    yanit = yanit_uret(ornek, model, tokenizer)\n",
        "    print(f\"Response:\\n{yanit}\")\n",
        "    print(\"-\" * 70)"
      ],
      "metadata": {
        "id": "3lrV75_AlX14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"kenhktsui/longtalk-cot-v0.1\")\n",
        "dataset[\"train\"][0]"
      ],
      "metadata": {
        "id": "67UiY-lbqrBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"train\"].shape"
      ],
      "metadata": {
        "id": "hDrgPIDwqtYF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}