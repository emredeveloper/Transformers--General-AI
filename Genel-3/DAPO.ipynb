{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef2277ee-1913-4142-821f-c3d371fb4291",
   "metadata": {},
   "source": [
    "1. Clip-Higher\n",
    "Clip-Higher, düşük olasılıklı token'ların olasılığını artırarak entropi çökmesini önlemek için kullanılır. Bu, modelin daha çeşitli cevaplar üretmesini sağlar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e70a1bf6-a628-4768-86f1-2ad4e84ac7dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orijinal Olasılıklar: [0.9  0.05 0.06]\n",
      "Soft Clip sonrası Olasılıklar: [0.30000369 0.19288612 0.19197462]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def soft_clip(probabilities, clip_low=0.2, clip_high=0.3, steepness=20):\n",
    "    \"\"\"\n",
    "    Yumuşak clip fonksiyonu:\n",
    "    - clip_low altında kalan değerler neredeyse clip_low'ya çekilir.\n",
    "    - clip_high üzerinde kalan değerler yavaşça bastırılır.\n",
    "    \"\"\"\n",
    "    # Yumuşak minimum\n",
    "    low_adjusted = clip_low + (probabilities - clip_low) / (1 + np.exp(-steepness*(probabilities - clip_low)))\n",
    "    # Yumuşak maksimum\n",
    "    high_adjusted = clip_high + (probabilities - clip_high) / (1 + np.exp(steepness*(probabilities - clip_high)))\n",
    "    # İki aşamada da uygulayarak dengeli sonuç elde ediyoruz\n",
    "    clipped = np.where(probabilities < clip_low, low_adjusted, probabilities)\n",
    "    clipped = np.where(probabilities > clip_high, high_adjusted, clipped)\n",
    "    return clipped\n",
    "\n",
    "# Örnek olasılık dağılımı\n",
    "probabilities = np.array([0.9, 0.05, 0.06])\n",
    "clipped_probabilities = soft_clip(probabilities)\n",
    "\n",
    "print(\"Orijinal Olasılıklar:\", probabilities)\n",
    "print(\"Soft Clip sonrası Olasılıklar:\", clipped_probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444b9ec8-0a19-4132-80c1-0c77b12362ed",
   "metadata": {},
   "source": [
    "2. Dinamik Örnekleme\n",
    "Dinamik Örnekleme, sıfır gradyanlı örnekleri filtreleyerek eğitim verimliliğini artırır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "32c04381-58ec-4cde-a4fb-63678bb007a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtrelenmiş Örnekler: [(0.4, 0), (0.55, None)]\n"
     ]
    }
   ],
   "source": [
    "# Örnek örnek seti: (olasılık, etiket) şeklinde\n",
    "samples = [(0.9, 1), (0.1, 0), (0.8, 1), (0.4, 0), (0.55, None)]\n",
    "\n",
    "def dynamic_sample_filter(samples, lower_bound=0.2, upper_bound=0.8):\n",
    "    \"\"\"\n",
    "    Dinamik örnekleme: \n",
    "    Sadece modelin kararsız kaldığı (olasılık değeri belirsiz aralıkta olan) örnekleri seçer.\n",
    "    \"\"\"\n",
    "    # Eğer etiket None ise sadece modelin tahminine göre filtre uyguluyoruz\n",
    "    filtered = [sample for sample in samples if lower_bound < sample[0] < upper_bound]\n",
    "    return filtered\n",
    "\n",
    "filtered_samples = dynamic_sample_filter(samples)\n",
    "print(\"Filtrelenmiş Örnekler:\", filtered_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93540ccb-1255-461f-974d-e1346438f9f0",
   "metadata": {},
   "source": [
    "3. Token-Level Policy Gradient Loss\n",
    "Bu teknik, her bir token'ın kaybını ayrı ayrı hesaplayarak uzun cümlelerdeki öğrenmeyi daha etkili hale getirir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "13841a52-33aa-40b2-bc33-0c15e59ad312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Düzeyinde Ağırlıklı Kayıp: 0.25333333333333335\n"
     ]
    }
   ],
   "source": [
    "# Örnek token dizisi ve token kayıpları (her token için hata)\n",
    "tokens = [\"token1\", \"token2\", \"token3\"]\n",
    "losses = [0.1, 0.4, 0.2]\n",
    "\n",
    "def token_policy_gradient_loss(tokens, losses, weights=None):\n",
    "    \"\"\"\n",
    "    Token-level kayıp hesaplama:\n",
    "    - Eğer weights belirtilmemişse tüm tokenlar eşit ağırlıkta kabul edilir.\n",
    "    - Ağırlıklı ortalama hesaplanır.\n",
    "    \"\"\"\n",
    "    if weights is None:\n",
    "        weights = [1.0] * len(tokens)\n",
    "    weighted_loss = sum(l * w for l, w in zip(losses, weights))\n",
    "    total_weight = sum(weights)\n",
    "    return weighted_loss / total_weight if total_weight != 0 else 0\n",
    "\n",
    "# Örneğin, daha kritik tokenlara yüksek ağırlık verilebilir:\n",
    "weights = [0.8, 1.2, 1.0]\n",
    "token_level_loss = token_policy_gradient_loss(tokens, losses, weights)\n",
    "print(\"Token Düzeyinde Ağırlıklı Kayıp:\", token_level_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbaed591-cce7-495e-a815-60912cd81386",
   "metadata": {},
   "source": [
    "4. Overlong Reward Shaping\n",
    "Bu teknik, çok uzun örnekler için ödül gürültüsünü azaltarak eğitim sürecini stabilize eder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "975b304b-d078-4188-8559-a6abc37b97e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Şekillendirilmiş Ödül: 1.9866142981514305\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def shaped_reward(length, base_reward=1, max_length=200, steepness=0.1):\n",
    "    \"\"\"\n",
    "    Overlong Reward Shaping:\n",
    "    - Uzunluk eşik değerini aşarsa, sigmoidal bir ceza uygulanır.\n",
    "    - Bu, uzunluk arttıkça cezanın yavaşça derinleşmesini sağlar.\n",
    "    \"\"\"\n",
    "    if length <= max_length:\n",
    "        return base_reward\n",
    "    # Sigmoid fonksiyon ile ceza: -1 + 2/(1+exp(-steepness*(length-max_length)))\n",
    "    penalty = -1 + 2 / (1 + math.exp(-steepness * (length - max_length)))\n",
    "    return base_reward + penalty\n",
    "\n",
    "# Örnek uzunluk ve ödül\n",
    "length = 250\n",
    "reward = shaped_reward(length)\n",
    "print(\"Şekillendirilmiş Ödül:\", reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "214a95e9-3a07-4ba4-a5ba-f6c1b55db122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Using cached datasets-3.4.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\emreq\\anaconda3\\lib\\site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\emreq\\anaconda3\\lib\\site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\emreq\\anaconda3\\lib\\site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\emreq\\anaconda3\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\emreq\\anaconda3\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\emreq\\anaconda3\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\emreq\\anaconda3\\lib\\site-packages (from datasets) (4.66.5)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp312-cp312-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in c:\\users\\emreq\\anaconda3\\lib\\site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\emreq\\anaconda3\\lib\\site-packages (from datasets) (3.10.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\emreq\\anaconda3\\lib\\site-packages (from datasets) (0.29.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\emreq\\anaconda3\\lib\\site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\emreq\\anaconda3\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\emreq\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\emreq\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\emreq\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\emreq\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\emreq\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\emreq\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.11.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\emreq\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.24.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\emreq\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\emreq\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\emreq\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\emreq\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
      "Requirement already satisfied: colorama in c:\\users\\emreq\\anaconda3\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\emreq\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\emreq\\anaconda3\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\emreq\\anaconda3\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\emreq\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Using cached datasets-3.4.1-py3-none-any.whl (487 kB)\n",
      "Downloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Downloading xxhash-3.5.0-cp312-cp312-win_amd64.whl (30 kB)\n",
      "Installing collected packages: xxhash, multiprocess, datasets\n",
      "Successfully installed datasets-3.4.1 multiprocess-0.70.16 xxhash-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d23e63-d9e5-4a23-9a63-3f6bb5fbe891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a779653fa9c405c89d346e046d679e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Loss: 8.1641\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    GPT2LMHeadModel,\n",
    "    GPT2TokenizerFast,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "from torch.optim import AdamW\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Model ve Tokenizer\n",
    "model_name = \"gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(model_name)\n",
    "\n",
    "# PAD Token Tanımlama\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Padding için EOS token'ı kullanıyoruz\n",
    "\n",
    "# Dataset Yükleme ve Tokenize Etme\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "dataset = dataset.select(range(2000))  # Küçük bir subset alalım\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True, max_length=256, padding=\"max_length\")\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "\n",
    "# Data Collator Kullanarak Padding İşlemi\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "dataloader = DataLoader(tokenized_dataset, batch_size=4, shuffle=True, collate_fn=data_collator)\n",
    "\n",
    "# Eğitim Parametreleri\n",
    "epochs = 3\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "num_training_steps = epochs * len(dataloader)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=100, num_training_steps=num_training_steps)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Eğitim Döngüsü\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        if step % 50 == 0:\n",
    "            print(f\"Step {step}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"Fine-tuning tamamlandı!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eafee14-c0e5-4ac9-8b2b-132f8712b7ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
