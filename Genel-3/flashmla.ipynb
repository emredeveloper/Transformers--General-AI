{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "def linear_attention_flasmla_inspired(q, k, v):\n",
    "    \"\"\"\n",
    "    FlashMLA-inspired Linear Attention (without flash-attn library)\n",
    "    \"\"\"\n",
    "    # Feature Map (elu + 1)\n",
    "    q = torch.nn.functional.elu(q) + 1\n",
    "    k = torch.nn.functional.elu(k) + 1\n",
    "\n",
    "    # Compute KV (K^T V first)\n",
    "    kv = torch.einsum(\"bhnd,bhmd->bhnm\", k, v)\n",
    "\n",
    "    # Compute normalization factors\n",
    "    q_sums = torch.einsum(\"bhnd->bhn\", q)\n",
    "    k_sums = torch.einsum(\"bhnd->bhn\", k)\n",
    "    kv_sums = torch.einsum(\"bhn,bhnm->bhm\", k_sums, kv)\n",
    "\n",
    "    # Compute Q(KV)\n",
    "    out = torch.einsum(\"bhnd,bhnm->bhmd\", q, kv)\n",
    "\n",
    "    # Normalize\n",
    "    normalizer = torch.einsum(\"bhn,bhm->bhm\", q_sums, kv_sums).unsqueeze(-1)\n",
    "    out = out / normalizer\n",
    "\n",
    "    return out\n",
    "\n",
    "def standard_attention(q, k, v, mask=None, softmax_scale=None):\n",
    "    \"\"\"\n",
    "    Standard Attention (Scaled Dot-Product Attention)\n",
    "    \"\"\"\n",
    "    d_k = q.size(-1)\n",
    "    if softmax_scale is None:\n",
    "        softmax_scale = 1.0 / (d_k ** 0.5)\n",
    "\n",
    "    scores = torch.matmul(q, k.transpose(-2, -1)) * softmax_scale\n",
    "\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "    attention_weights = torch.softmax(scores, dim=-1)\n",
    "    out = torch.matmul(attention_weights, v)\n",
    "\n",
    "    return out, attention_weights\n",
    "\n",
    "def compare_attention(batch_size, num_heads, seq_len_q, seq_len_k, head_dim, device=\"cuda\", num_iters=100, causal=False):\n",
    "    \"\"\"\n",
    "    Compare Standard Attention and FlashMLA-inspired Linear Attention\n",
    "    \"\"\"\n",
    "    q = torch.randn(batch_size, num_heads, seq_len_q, head_dim, device=device)\n",
    "    k = torch.randn(batch_size, num_heads, seq_len_k, head_dim, device=device)\n",
    "    v = torch.randn(batch_size, num_heads, seq_len_k, head_dim, device=device)\n",
    "\n",
    "    mask = None\n",
    "    if causal:\n",
    "        mask = torch.tril(torch.ones(seq_len_q, seq_len_q, device=device, dtype=torch.bool))\n",
    "        mask = mask.view(1, 1, seq_len_q, seq_len_q)\n",
    "\n",
    "    # Warmup\n",
    "    for _ in range(5):\n",
    "        with torch.no_grad():\n",
    "            _ = standard_attention(q, k, v, mask)\n",
    "            _ = linear_attention_flasmla_inspired(q, k, v)\n",
    "\n",
    "    torch.cuda.synchronize() if device == \"cuda\" else None\n",
    "\n",
    "    # Timing for Standard Attention\n",
    "    if device == \"cuda\":\n",
    "        start = torch.cuda.Event(enable_timing=True)\n",
    "        end = torch.cuda.Event(enable_timing=True)\n",
    "        \n",
    "        start.record()\n",
    "        for _ in range(num_iters):\n",
    "            with torch.no_grad():\n",
    "                _ = standard_attention(q, k, v, mask)\n",
    "        end.record()\n",
    "        torch.cuda.synchronize()\n",
    "        std_time = start.elapsed_time(end) / num_iters\n",
    "    else:\n",
    "        start_time = time.perf_counter()\n",
    "        for _ in range(num_iters):\n",
    "            with torch.no_grad():\n",
    "                _ = standard_attention(q, k, v, mask)\n",
    "        std_time = (time.perf_counter() - start_time) / num_iters * 1000\n",
    "\n",
    "    # Timing for Linear Attention\n",
    "    if device == \"cuda\":\n",
    "        start.record()\n",
    "        for _ in range(num_iters):\n",
    "            with torch.no_grad():\n",
    "                _ = linear_attention_flasmla_inspired(q, k, v)\n",
    "        end.record()\n",
    "        torch.cuda.synchronize()\n",
    "        linear_time = start.elapsed_time(end) / num_iters\n",
    "    else:\n",
    "        start_time = time.perf_counter()\n",
    "        for _ in range(num_iters):\n",
    "            with torch.no_grad():\n",
    "                _ = linear_attention_flasmla_inspired(q, k, v)\n",
    "        linear_time = (time.perf_counter() - start_time) / num_iters * 1000\n",
    "\n",
    "    return std_time, linear_time\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Device: {device}\")\n",
    "\n",
    "    batch_size = 32\n",
    "    num_heads = 8\n",
    "    head_dim = 64\n",
    "\n",
    "    results = {}\n",
    "    sequence_lengths = [(64, 64), (128, 128), (256, 256), (512, 512), \n",
    "                       (1024, 1024), (2048, 2048), (4096, 4096)]\n",
    "\n",
    "    for seq_len_q, seq_len_k in sequence_lengths:\n",
    "        if seq_len_q == seq_len_k:\n",
    "            std_time, linear_time = compare_attention(\n",
    "                batch_size, num_heads, seq_len_q, seq_len_k, \n",
    "                head_dim, device, causal=True\n",
    "            )\n",
    "            results[(seq_len_q, seq_len_k, True)] = (std_time, linear_time)\n",
    "\n",
    "        std_time, linear_time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
