{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import time\n",
    "import tracemalloc\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Add your Hugging Face token here (you can get it from https://huggingface.co/settings/tokens)\n",
    "HUGGINGFACE_TOKEN = \"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\"  # Replace with your actual token\n",
    "\n",
    "# Login to Hugging Face\n",
    "try:\n",
    "    login(token=HUGGINGFACE_TOKEN)\n",
    "    print(\"Successfully logged in to Hugging Face\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to login to Hugging Face: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import time\n",
    "import tracemalloc\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Multi-head Attention (MHA) sınıfı\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        Q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_k ** 0.5)\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        context = torch.matmul(attn, V)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        output = self.W_o(context)\n",
    "        return output\n",
    "\n",
    "# Multi-head Latent Attention (MLA) sınıfı\n",
    "class MultiHeadLatentAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, latent_dim):\n",
    "        super(MultiHeadLatentAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_kv_down = nn.Linear(d_model, latent_dim)\n",
    "        self.W_k_up = nn.Linear(latent_dim, d_model)\n",
    "        self.W_v_up = nn.Linear(latent_dim, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        Q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        kv_latent = self.W_kv_down(x)\n",
    "        K = self.W_k_up(kv_latent).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v_up(kv_latent).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_k ** 0.5)\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        context = torch.matmul(attn, V)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        output = self.W_o(context)\n",
    "        return output\n",
    "\n",
    "# Veri setini yükleme ve ön işlem (Düzeltildi)\n",
    "def prepare_data(batch_size=32, seq_len=128, d_model=512):\n",
    "    # IMDB veri setini yükle\n",
    "    dataset = load_dataset(\"imdb\", split=\"train[:1000]\")  # İlk 1000 örnek\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    \n",
    "    # Metinleri token'ize et ve tensöre çevir\n",
    "    inputs = tokenizer(\n",
    "        dataset[\"text\"], \n",
    "        max_length=seq_len, \n",
    "        padding=\"max_length\", \n",
    "        truncation=True, \n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Embedding katmanı ile token'ları d_model boyutuna çevir\n",
    "    embedding = nn.Embedding(tokenizer.vocab_size, d_model)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    data = embedding(input_ids)\n",
    "    \n",
    "    # Batch'lere ayırırken tam bölünme sağla\n",
    "    num_samples = data.size(0)\n",
    "    num_batches = num_samples // batch_size\n",
    "    data = data[:num_batches * batch_size]  # Kalan örnekleri düşür\n",
    "    data = data.view(num_batches, batch_size, seq_len, d_model)\n",
    "    \n",
    "    print(f\"Veri boyutu: {data.shape} (num_batches, batch_size, seq_len, d_model)\")\n",
    "    return data\n",
    "\n",
    "# Performans testi\n",
    "def run_performance_test(data, d_model=512, num_heads=8, latent_dims=[128, 64]):\n",
    "    mha = MultiHeadAttention(d_model, num_heads)\n",
    "    mla_models = {dim: MultiHeadLatentAttention(d_model, num_heads, dim) for dim in latent_dims}\n",
    "    \n",
    "    results = {\"MHA\": {\"time\": [], \"memory\": [], \"output\": None}}\n",
    "    for dim in latent_dims:\n",
    "        results[f\"MLA_{dim}\"] = {\"time\": [], \"memory\": [], \"output\": None}\n",
    "    \n",
    "    for batch in tqdm(data, desc=\"Batch'ler üzerinde test\"):\n",
    "        # MHA için\n",
    "        tracemalloc.start()\n",
    "        start_time = time.time()\n",
    "        mha_output = mha(batch)\n",
    "        end_time = time.time()\n",
    "        _, peak_memory = tracemalloc.get_traced_memory()\n",
    "        tracemalloc.stop()\n",
    "        results[\"MHA\"][\"time\"].append(end_time - start_time)\n",
    "        results[\"MHA\"][\"memory\"].append(peak_memory / 1024 / 1024)\n",
    "        results[\"MHA\"][\"output\"] = mha_output\n",
    "        \n",
    "        # MLA için farklı latent_dim'ler\n",
    "        for dim, mla in mla_models.items():\n",
    "            tracemalloc.start()\n",
    "            start_time = time.time()\n",
    "            mla_output = mla(batch)\n",
    "            end_time = time.time()\n",
    "            _, peak_memory = tracemalloc.get_traced_memory()\n",
    "            tracemalloc.stop()\n",
    "            results[f\"MLA_{dim}\"][\"time\"].append(end_time - start_time)\n",
    "            results[f\"MLA_{dim}\"][\"memory\"].append(peak_memory / 1024 / 1024)\n",
    "            results[f\"MLA_{dim}\"][\"output\"] = mla_output\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Sonuçları görselleştirme\n",
    "def visualize_results(results):\n",
    "    labels = list(results.keys())\n",
    "    avg_times = [sum(results[label][\"time\"]) / len(results[label][\"time\"]) for label in labels]\n",
    "    avg_memories = [sum(results[label][\"memory\"]) / len(results[label][\"memory\"]) for label in labels]\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.bar(labels, avg_times, color=[\"blue\", \"orange\", \"green\"])\n",
    "    plt.title(\"Ortalama Çalışma Süresi (saniye)\")\n",
    "    plt.ylabel(\"Süre (s)\")\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.bar(labels, avg_memories, color=[\"blue\", \"orange\", \"green\"])\n",
    "    plt.title(\"Ortalama Bellek Kullanımı (MB)\")\n",
    "    plt.ylabel(\"Bellek (MB)\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    mha_out = results[\"MHA\"][\"output\"]\n",
    "    for label in labels[1:]:\n",
    "        mla_out = results[label][\"output\"]\n",
    "        diff = torch.mean(torch.abs(mha_out - mla_out)).item()\n",
    "        print(f\"{label} ile MHA arasındaki ortalama çıktı farkı: {diff:.6f}\")\n",
    "\n",
    "# Ana fonksiyon\n",
    "def main():\n",
    "    print(\"Veri seti hazırlanıyor...\")\n",
    "    data = prepare_data(batch_size=32, seq_len=128, d_model=512)\n",
    "    \n",
    "    print(\"Performans testi yapılıyor...\")\n",
    "    results = run_performance_test(data, d_model=512, num_heads=8, latent_dims=[128, 64])\n",
    "    \n",
    "    print(\"\\n=== Performans Sonuçları ===\")\n",
    "    for label in results:\n",
    "        avg_time = sum(results[label][\"time\"]) / len(results[label][\"time\"])\n",
    "        avg_memory = sum(results[label][\"memory\"]) / len(results[label][\"memory\"])\n",
    "        print(f\"{label}:\")\n",
    "        print(f\" - Ortalama Çalışma Süresi: {avg_time:.4f} saniye\")\n",
    "        print(f\" - Ortalama Bellek Kullanımı: {avg_memory:.2f} MB\")\n",
    "    \n",
    "    print(\"\\nGörselleştirme yapılıyor...\")\n",
    "    visualize_results(results)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import time\n",
    "import tracemalloc\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Multi-head Attention (MHA)\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        Q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_k ** 0.5)\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        context = torch.matmul(attn, V)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        return self.W_o(context)\n",
    "\n",
    "# Multi-head Latent Attention (MLA)\n",
    "class MultiHeadLatentAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, latent_dim):\n",
    "        super(MultiHeadLatentAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_kv_down = nn.Linear(d_model, latent_dim)\n",
    "        self.W_k_up = nn.Linear(latent_dim, d_model)\n",
    "        self.W_v_up = nn.Linear(latent_dim, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        Q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        kv_latent = self.W_kv_down(x)\n",
    "        K = self.W_k_up(kv_latent).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v_up(kv_latent).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_k ** 0.5)\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        context = torch.matmul(attn, V)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        return self.W_o(context)\n",
    "\n",
    "# Sınıflandırma modeli\n",
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, attention_type=\"MHA\", latent_dim=None):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(30522, d_model)\n",
    "        if attention_type == \"MHA\":\n",
    "            self.attention = MultiHeadAttention(d_model, num_heads)\n",
    "        elif attention_type == \"MLA\":\n",
    "            self.attention = MultiHeadLatentAttention(d_model, num_heads, latent_dim)\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(d_model, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.attention(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.pool(x).squeeze(-1)\n",
    "        return self.fc(x)\n",
    "\n",
    "# Parametre sayısını hesaplama\n",
    "def count_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total_params, trainable_params\n",
    "\n",
    "# Veri setini hazırlama\n",
    "def prepare_data(batch_size=32, seq_len=128):\n",
    "    dataset = load_dataset(\"imdb\", split=\"train[:1000]\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        dataset[\"text\"], \n",
    "        max_length=seq_len, \n",
    "        padding=\"max_length\", \n",
    "        truncation=True, \n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    labels = torch.tensor(dataset[\"label\"])\n",
    "    \n",
    "    train_size = 800\n",
    "    train_data = input_ids[:train_size]\n",
    "    train_labels = labels[:train_size]\n",
    "    test_data = input_ids[train_size:]\n",
    "    test_labels = labels[train_size:]\n",
    "    \n",
    "    num_train_batches = train_size // batch_size\n",
    "    train_data = train_data[:num_train_batches * batch_size].view(num_train_batches, batch_size, seq_len)\n",
    "    train_labels = train_labels[:num_train_batches * batch_size].view(num_train_batches, batch_size)\n",
    "    \n",
    "    num_test_samples = test_data.size(0)\n",
    "    num_test_batches = num_test_samples // batch_size\n",
    "    test_data = test_data[:num_test_batches * batch_size].view(num_test_batches, batch_size, seq_len)\n",
    "    test_labels = test_labels[:num_test_batches * batch_size].view(num_test_batches, batch_size)\n",
    "    \n",
    "    print(f\"Eğitim veri boyutu: {train_data.shape}\")\n",
    "    print(f\"Test veri boyutu: {test_data.shape}\")\n",
    "    return (train_data, train_labels), (test_data, test_labels)\n",
    "\n",
    "# Modeli eğitme (Detaylı çıktı)\n",
    "def train_model(model, train_data, train_labels, epochs=5, lr=0.001):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    results = {\"loss\": [], \"accuracy\": [], \"time\": [], \"memory\": []}\n",
    "    for epoch in tqdm(range(epochs), desc=\"Epochs\"):\n",
    "        epoch_loss = 0\n",
    "        epoch_correct = 0\n",
    "        epoch_time = 0\n",
    "        epoch_memory = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        for batch_data, batch_labels in zip(train_data, train_labels):\n",
    "            tracemalloc.start()\n",
    "            start_time = time.time()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_data)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            end_time = time.time()\n",
    "            _, peak_memory = tracemalloc.get_traced_memory()\n",
    "            tracemalloc.stop()\n",
    "            \n",
    "            # Batch bazında doğruluk\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct = (predicted == batch_labels).sum().item()\n",
    "            batch_size = batch_labels.size(0)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_correct += correct\n",
    "            epoch_time += end_time - start_time\n",
    "            epoch_memory += peak_memory / 1024 / 1024\n",
    "            total_samples += batch_size\n",
    "        \n",
    "        avg_loss = epoch_loss / len(train_data)\n",
    "        accuracy = epoch_correct / total_samples\n",
    "        avg_time = epoch_time / len(train_data)\n",
    "        avg_memory = epoch_memory / len(train_data)\n",
    "        \n",
    "        results[\"loss\"].append(avg_loss)\n",
    "        results[\"accuracy\"].append(accuracy)\n",
    "        results[\"time\"].append(avg_time)\n",
    "        results[\"memory\"].append(avg_memory)\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs}:\")\n",
    "        print(f\" - Ortalama Kayıp: {avg_loss:.4f}\")\n",
    "        print(f\" - Doğruluk: {accuracy:.4f}\")\n",
    "        print(f\" - Ortalama Batch Süresi: {avg_time:.4f} saniye\")\n",
    "        print(f\" - Ortalama Bellek Kullanımı: {avg_memory:.2f} MB\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test etme (Detaylı çıktı)\n",
    "def test_model(model, test_data, test_labels):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_loss = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_data, batch_labels in zip(test_data, test_labels):\n",
    "            outputs = model(batch_data)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += batch_labels.size(0)\n",
    "            correct += (predicted == batch_labels).sum().item()\n",
    "            test_loss += loss.item()\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    avg_loss = test_loss / len(test_data)\n",
    "    print(f\"\\nTest Sonuçları:\")\n",
    "    print(f\" - Ortalama Kayıp: {avg_loss:.4f}\")\n",
    "    print(f\" - Doğruluk: {accuracy:.4f}\")\n",
    "    print(f\" - Toplam Örnek Sayısı: {total}\")\n",
    "    return accuracy, avg_loss\n",
    "\n",
    "# Ana fonksiyon\n",
    "def main():\n",
    "    batch_size, seq_len, d_model, num_heads = 32, 128, 512, 8\n",
    "    latent_dim = 128\n",
    "    \n",
    "    print(\"Veri seti hazırlanıyor...\")\n",
    "    (train_data, train_labels), (test_data, test_labels) = prepare_data(batch_size, seq_len)\n",
    "    \n",
    "    # Modelleri oluştur\n",
    "    print(\"\\n=== MHA Modeli ===\")\n",
    "    mha_model = SentimentClassifier(d_model, num_heads, \"MHA\")\n",
    "    mha_total_params, mha_trainable_params = count_parameters(mha_model)\n",
    "    print(f\"Toplam Parametre Sayısı: {mha_total_params:,}\")\n",
    "    print(f\"Eğitilebilir Parametre Sayısı: {mha_trainable_params:,}\")\n",
    "    \n",
    "    print(\"\\n=== MLA Modeli ===\")\n",
    "    mla_model = SentimentClassifier(d_model, num_heads, \"MLA\", latent_dim)\n",
    "    mla_total_params, mla_trainable_params = count_parameters(mla_model)\n",
    "    print(f\"Toplam Parametre Sayısı: {mla_total_params:,}\")\n",
    "    print(f\"Eğitilebilir Parametre Sayısı: {mla_trainable_params:,}\")\n",
    "    \n",
    "    print(\"\\nMHA modeli eğitiliyor...\")\n",
    "    mha_results = train_model(mha_model, train_data, train_labels)\n",
    "    mha_accuracy, mha_test_loss = test_model(mha_model, test_data, test_labels)\n",
    "    \n",
    "    print(\"\\nMLA modeli eğitiliyor...\")\n",
    "    mla_results = train_model(mla_model, train_data, train_labels)\n",
    "    mla_accuracy, mla_test_loss = test_model(mla_model, test_data, test_labels)\n",
    "    \n",
    "    # Sonuçları görselleştir\n",
    "    epochs = range(1, 6)\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(epochs, mha_results[\"loss\"], label=\"MHA\")\n",
    "    plt.plot(epochs, mla_results[\"loss\"], label=\"MLA\")\n",
    "    plt.title(\"Eğitim Kaybı\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(epochs, mha_results[\"accuracy\"], label=\"MHA\")\n",
    "    plt.plot(epochs, mla_results[\"accuracy\"], label=\"MLA\")\n",
    "    plt.title(\"Eğitim Doğruluğu\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(epochs, mha_results[\"time\"], label=\"MHA\")\n",
    "    plt.plot(epochs, mla_results[\"time\"], label=\"MLA\")\n",
    "    plt.title(\"Ortalama Batch Süresi (s)\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Süre (s)\")\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(epochs, mha_results[\"memory\"], label=\"MHA\")\n",
    "    plt.plot(epochs, mla_results[\"memory\"], label=\"MLA\")\n",
    "    plt.title(\"Ortalama Bellek Kullanımı (MB)\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Bellek (MB)\")\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import time\n",
    "import tracemalloc\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Multi-head Attention (MHA)\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        Q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_k ** 0.5)\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        context = torch.matmul(attn, V)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        return self.W_o(context)\n",
    "\n",
    "# Multi-head Latent Attention (MLA)\n",
    "class MultiHeadLatentAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, latent_dim):\n",
    "        super(MultiHeadLatentAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_kv_down = nn.Linear(d_model, latent_dim)\n",
    "        self.W_k_up = nn.Linear(latent_dim, d_model)\n",
    "        self.W_v_up = nn.Linear(latent_dim, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        Q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        kv_latent = self.W_kv_down(x)\n",
    "        K = self.W_k_up(kv_latent).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v_up(kv_latent).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_k ** 0.5)\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        context = torch.matmul(attn, V)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        return self.W_o(context)\n",
    "\n",
    "# Sınıflandırma modeli\n",
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, attention_type=\"MHA\", latent_dim=None):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(30522, d_model)\n",
    "        if attention_type == \"MHA\":\n",
    "            self.attention = MultiHeadAttention(d_model, num_heads)\n",
    "        elif attention_type == \"MLA\":\n",
    "            self.attention = MultiHeadLatentAttention(d_model, num_heads, latent_dim)\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(d_model, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.attention(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.pool(x).squeeze(-1)\n",
    "        return self.fc(x)\n",
    "\n",
    "# Parametre sayısını hesaplama\n",
    "def count_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total_params, trainable_params\n",
    "\n",
    "# Veri setini hazırlama (Validasyon seti eklendi)\n",
    "def prepare_data(batch_size=32, seq_len=128):\n",
    "    dataset = load_dataset(\"imdb\", split=\"train[:1000]\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        dataset[\"text\"], \n",
    "        max_length=seq_len, \n",
    "        padding=\"max_length\", \n",
    "        truncation=True, \n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    labels = torch.tensor(dataset[\"label\"])\n",
    "    \n",
    "    # Eğitim, validasyon ve test setine ayır\n",
    "    train_size = 700\n",
    "    val_size = 100\n",
    "    test_size = 200\n",
    "    \n",
    "    train_data = input_ids[:train_size]\n",
    "    train_labels = labels[:train_size]\n",
    "    val_data = input_ids[train_size:train_size+val_size]\n",
    "    val_labels = labels[train_size:train_size+val_size]\n",
    "    test_data = input_ids[train_size+val_size:]\n",
    "    test_labels = labels[train_size+val_size:]\n",
    "    \n",
    "    # Batch'lere ayır\n",
    "    num_train_batches = train_size // batch_size\n",
    "    train_data = train_data[:num_train_batches * batch_size].view(num_train_batches, batch_size, seq_len)\n",
    "    train_labels = train_labels[:num_train_batches * batch_size].view(num_train_batches, batch_size)\n",
    "    \n",
    "    num_val_batches = val_size // batch_size\n",
    "    val_data = val_data[:num_val_batches * batch_size].view(num_val_batches, batch_size, seq_len)\n",
    "    val_labels = val_labels[:num_val_batches * batch_size].view(num_val_batches, batch_size)\n",
    "    \n",
    "    num_test_batches = test_size // batch_size\n",
    "    test_data = test_data[:num_test_batches * batch_size].view(num_test_batches, batch_size, seq_len)\n",
    "    test_labels = test_labels[:num_test_batches * batch_size].view(num_test_batches, batch_size)\n",
    "    \n",
    "    print(f\"Eğitim veri boyutu: {train_data.shape}\")\n",
    "    print(f\"Validasyon veri boyutu: {val_data.shape}\")\n",
    "    print(f\"Test veri boyutu: {test_data.shape}\")\n",
    "    return (train_data, train_labels), (val_data, val_labels), (test_data, test_labels)\n",
    "\n",
    "# Modeli eğitme ve validasyon (Batch bazında detaylı)\n",
    "def train_model(model, train_data, train_labels, val_data, val_labels, epochs=5, lr=0.001):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    results = {\n",
    "        \"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": [],\n",
    "        \"time\": [], \"memory\": []\n",
    "    }\n",
    "    \n",
    "    for epoch in tqdm(range(epochs), desc=\"Epochs\"):\n",
    "        model.train()\n",
    "        epoch_train_loss = 0\n",
    "        epoch_train_correct = 0\n",
    "        epoch_time = 0\n",
    "        epoch_memory = 0\n",
    "        total_train_samples = 0\n",
    "        \n",
    "        print(f\"\\n=== Epoch {epoch+1}/{epochs} - Eğitim ===\")\n",
    "        for i, (batch_data, batch_labels) in enumerate(zip(train_data, train_labels)):\n",
    "            tracemalloc.start()\n",
    "            start_time = time.time()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_data)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            end_time = time.time()\n",
    "            _, peak_memory = tracemalloc.get_traced_memory()\n",
    "            tracemalloc.stop()\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct = (predicted == batch_labels).sum().item()\n",
    "            batch_size = batch_labels.size(0)\n",
    "            \n",
    "            epoch_train_loss += loss.item()\n",
    "            epoch_train_correct += correct\n",
    "            epoch_time += end_time - start_time\n",
    "            epoch_memory += peak_memory / 1024 / 1024\n",
    "            total_train_samples += batch_size\n",
    "            \n",
    "            print(f\"Batch {i+1}/{len(train_data)} - Kayıp: {loss.item():.4f} - Doğruluk: {correct/batch_size:.4f}\")\n",
    "        \n",
    "        # Validasyon\n",
    "        model.eval()\n",
    "        epoch_val_loss = 0\n",
    "        epoch_val_correct = 0\n",
    "        total_val_samples = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_data, batch_labels in zip(val_data, val_labels):\n",
    "                outputs = model(batch_data)\n",
    "                loss = criterion(outputs, batch_labels)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct = (predicted == batch_labels).sum().item()\n",
    "                batch_size = batch_labels.size(0)\n",
    "                \n",
    "                epoch_val_loss += loss.item()\n",
    "                epoch_val_correct += correct\n",
    "                total_val_samples += batch_size\n",
    "        \n",
    "        # Ortalamaları hesapla\n",
    "        train_loss = epoch_train_loss / len(train_data)\n",
    "        train_acc = epoch_train_correct / total_train_samples\n",
    "        val_loss = epoch_val_loss / len(val_data)\n",
    "        val_acc = epoch_val_correct / total_val_samples\n",
    "        avg_time = epoch_time / len(train_data)\n",
    "        avg_memory = epoch_memory / len(train_data)\n",
    "        \n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"val_loss\"].append(val_loss)\n",
    "        results[\"val_acc\"].append(val_acc)\n",
    "        results[\"time\"].append(avg_time)\n",
    "        results[\"memory\"].append(avg_memory)\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs} Özeti:\")\n",
    "        print(f\" - Eğitim Kayıp: {train_loss:.4f} - Eğitim Doğruluk: {train_acc:.4f}\")\n",
    "        print(f\" - Validasyon Kayıp: {val_loss:.4f} - Validasyon Doğruluk: {val_acc:.4f}\")\n",
    "        print(f\" - Ortalama Batch Süresi: {avg_time:.4f} saniye\")\n",
    "        print(f\" - Ortalama Bellek Kullanımı: {avg_memory:.2f} MB\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test etme\n",
    "def test_model(model, test_data, test_labels):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_loss = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (batch_data, batch_labels) in enumerate(zip(test_data, test_labels)):\n",
    "            outputs = model(batch_data)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += batch_labels.size(0)\n",
    "            correct += (predicted == batch_labels).sum().item()\n",
    "            test_loss += loss.item()\n",
    "            print(f\"Test Batch {i+1}/{len(test_data)} - Kayıp: {loss.item():.4f} - Doğruluk: {(predicted == batch_labels).sum().item()/batch_labels.size(0):.4f}\")\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    avg_loss = test_loss / len(test_data)\n",
    "    print(f\"\\nTest Özeti:\")\n",
    "    print(f\" - Ortalama Kayıp: {avg_loss:.4f}\")\n",
    "    print(f\" - Doğruluk: {accuracy:.4f}\")\n",
    "    print(f\" - Toplam Örnek Sayısı: {total}\")\n",
    "    return accuracy, avg_loss\n",
    "\n",
    "# Ana fonksiyon\n",
    "def main():\n",
    "    # Hiperparametreler\n",
    "    batch_size, seq_len, d_model, num_heads = 32, 128, 512, 8\n",
    "    epochs = 1\n",
    "    lr = 0.0001  # Daha yavaş öğrenme için düşürüldü\n",
    "    latent_dims = [64, 128, 256]  # Farklı latent_dim değerleri\n",
    "    \n",
    "    print(\"Veri seti hazırlanıyor...\")\n",
    "    (train_data, train_labels), (val_data, val_labels), (test_data, test_labels) = prepare_data(batch_size, seq_len)\n",
    "    \n",
    "    # Hiperparametreleri yazdır\n",
    "    print(\"\\n=== Hiperparametreler ===\")\n",
    "    print(f\"Batch Size: {batch_size}\")\n",
    "    print(f\"Sequence Length: {seq_len}\")\n",
    "    print(f\"Model Dimension: {d_model}\")\n",
    "    print(f\"Number of Heads: {num_heads}\")\n",
    "    print(f\"Epochs: {epochs}\")\n",
    "    print(f\"Learning Rate: {lr}\")\n",
    "    \n",
    "    # MHA Modeli\n",
    "    print(\"\\n=== MHA Modeli ===\")\n",
    "    mha_model = SentimentClassifier(d_model, num_heads, \"MHA\")\n",
    "    mha_total_params, mha_trainable_params = count_parameters(mha_model)\n",
    "    print(f\"Toplam Parametre Sayısı: {mha_total_params:,}\")\n",
    "    print(f\"Eğitilebilir Parametre Sayısı: {mha_trainable_params:,}\")\n",
    "    \n",
    "    print(\"\\nMHA modeli eğitiliyor...\")\n",
    "    mha_results = train_model(mha_model, train_data, train_labels, val_data, val_labels, epochs, lr)\n",
    "    mha_accuracy, mha_test_loss = test_model(mha_model, test_data, test_labels)\n",
    "    \n",
    "    # MLA Modelleri (farklı latent_dim'ler)\n",
    "    mla_results = {}\n",
    "    for latent_dim in latent_dims:\n",
    "        print(f\"\\n=== MLA Modeli (latent_dim={latent_dim}) ===\")\n",
    "        mla_model = SentimentClassifier(d_model, num_heads, \"MLA\", latent_dim)\n",
    "        mla_total_params, mla_trainable_params = count_parameters(mla_model)\n",
    "        print(f\"Toplam Parametre Sayısı: {mla_total_params:,}\")\n",
    "        print(f\"Eğitilebilir Parametre Sayısı: {mla_trainable_params:,}\")\n",
    "        \n",
    "        print(f\"\\nMLA modeli (latent_dim={latent_dim}) eğitiliyor...\")\n",
    "        mla_results[latent_dim] = train_model(mla_model, train_data, train_labels, val_data, val_labels, epochs, lr)\n",
    "        mla_accuracy, mla_test_loss = test_model(mla_model, test_data, test_labels)\n",
    "        mla_results[latent_dim][\"test_acc\"] = mla_accuracy\n",
    "        mla_results[latent_dim][\"test_loss\"] = mla_test_loss\n",
    "    \n",
    "    # Sonuçları görselleştir\n",
    "    epochs_range = range(1, epochs + 1)\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    \n",
    "    # Eğitim Kaybı\n",
    "    plt.subplot(3, 2, 1)\n",
    "    plt.plot(epochs_range, mha_results[\"train_loss\"], label=\"MHA\")\n",
    "    for latent_dim in latent_dims:\n",
    "        plt.plot(epochs_range, mla_results[latent_dim][\"train_loss\"], label=f\"MLA_{latent_dim}\")\n",
    "    plt.title(\"Eğitim Kaybı\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    \n",
    "    # Eğitim Doğruluğu\n",
    "    plt.subplot(3, 2, 2)\n",
    "    plt.plot(epochs_range, mha_results[\"train_acc\"], label=\"MHA\")\n",
    "    for latent_dim in latent_dims:\n",
    "        plt.plot(epochs_range, mla_results[latent_dim][\"train_acc\"], label=f\"MLA_{latent_dim}\")\n",
    "    plt.title(\"Eğitim Doğruluğu\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    \n",
    "    # Validasyon Kaybı\n",
    "    plt.subplot(3, 2, 3)\n",
    "    plt.plot(epochs_range, mha_results[\"val_loss\"], label=\"MHA\")\n",
    "    for latent_dim in latent_dims:\n",
    "        plt.plot(epochs_range, mla_results[latent_dim][\"val_loss\"], label=f\"MLA_{latent_dim}\")\n",
    "    plt.title(\"Validasyon Kaybı\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    \n",
    "    # Validasyon Doğruluğu\n",
    "    plt.subplot(3, 2, 4)\n",
    "    plt.plot(epochs_range, mha_results[\"val_acc\"], label=\"MHA\")\n",
    "    for latent_dim in latent_dims:\n",
    "        plt.plot(epochs_range, mla_results[latent_dim][\"val_acc\"], label=f\"MLA_{latent_dim}\")\n",
    "    plt.title(\"Validasyon Doğruluğu\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    \n",
    "    # Ortalama Batch Süresi\n",
    "    plt.subplot(3, 2, 5)\n",
    "    plt.plot(epochs_range, mha_results[\"time\"], label=\"MHA\")\n",
    "    for latent_dim in latent_dims:\n",
    "        plt.plot(epochs_range, mla_results[latent_dim][\"time\"], label=f\"MLA_{latent_dim}\")\n",
    "    plt.title(\"Ortalama Batch Süresi (s)\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Süre (s)\")\n",
    "    plt.legend()\n",
    "    \n",
    "    # Ortalama Bellek Kullanımı\n",
    "    plt.subplot(3, 2, 6)\n",
    "    plt.plot(epochs_range, mha_results[\"memory\"], label=\"MHA\")\n",
    "    for latent_dim in latent_dims:\n",
    "        plt.plot(epochs_range, mla_results[latent_dim][\"memory\"], label=f\"MLA_{latent_dim}\")\n",
    "    plt.title(\"Ortalama Bellek Kullanımı (MB)\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Bellek (MB)\")\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import time\n",
    "import tracemalloc\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# RoPE için düzeltilmiş yardımcı fonksiyon\n",
    "def apply_rotary_pos_emb(q, k, seq_len, dim):\n",
    "    theta = torch.arange(0, dim, 2, dtype=torch.float, device=q.device) / dim\n",
    "    theta = 10000 ** (-theta)\n",
    "    positions = torch.arange(seq_len, dtype=torch.float, device=q.device).unsqueeze(1)\n",
    "    angles = positions * theta.unsqueeze(0)\n",
    "    sin_angles = torch.sin(angles).unsqueeze(0).unsqueeze(0)  # [1, 1, seq_len, dim/2]\n",
    "    cos_angles = torch.cos(angles).unsqueeze(0).unsqueeze(0)\n",
    "    \n",
    "    # q ve k'nın yarısını döndürmek için ayır\n",
    "    q_reshape = q.reshape(*q.shape[:-1], -1, 2)  # [batch, heads, seq_len, d_k/2, 2]\n",
    "    k_reshape = k.reshape(*k.shape[:-1], -1, 2)\n",
    "    \n",
    "    q_rot = torch.cat([\n",
    "        q_reshape[..., 0] * cos_angles - q_reshape[..., 1] * sin_angles,\n",
    "        q_reshape[..., 0] * sin_angles + q_reshape[..., 1] * cos_angles\n",
    "    ], dim=-1)\n",
    "    k_rot = torch.cat([\n",
    "        k_reshape[..., 0] * cos_angles - k_reshape[..., 1] * sin_angles,\n",
    "        k_reshape[..., 0] * sin_angles + k_reshape[..., 1] * cos_angles\n",
    "    ], dim=-1)\n",
    "    \n",
    "    return q_rot.view_as(q), k_rot.view_as(k)\n",
    "\n",
    "# Multi-head Attention (MHA)\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, use_rope=False, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.use_rope = use_rope\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        Q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        if self.use_rope:\n",
    "            Q, K = apply_rotary_pos_emb(Q, K, seq_len, self.d_k)\n",
    "        \n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_k ** 0.5)\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        context = torch.matmul(attn, V)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        return self.W_o(context)\n",
    "\n",
    "# Multi-head Latent Attention (MLA)\n",
    "class MultiHeadLatentAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, latent_dim, use_rope=False, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.latent_dim = latent_dim\n",
    "        self.use_rope = use_rope\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_kv_down = nn.Linear(d_model, latent_dim)\n",
    "        self.W_k_up = nn.Linear(latent_dim, d_model)\n",
    "        self.W_v_up = nn.Linear(latent_dim, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        Q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        kv_latent = self.W_kv_down(x)\n",
    "        K = self.W_k_up(kv_latent).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v_up(kv_latent).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        if self.use_rope:\n",
    "            Q, K = apply_rotary_pos_emb(Q, K, seq_len, self.d_k)\n",
    "        \n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_k ** 0.5)\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        context = torch.matmul(attn, V)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        return self.W_o(context)\n",
    "\n",
    "# Longformer Attention (Global token desteğiyle geliştirilmiş)\n",
    "class LongformerAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, window_size=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.window_size = window_size\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        Q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # CLS token (ilk token) global olsun\n",
    "        scores = torch.zeros(batch_size, self.num_heads, seq_len, seq_len, device=x.device)\n",
    "        for i in range(seq_len):\n",
    "            if i == 0:  # CLS token global dikkat alır\n",
    "                scores[:, :, 0, :] = torch.matmul(Q[:, :, 0:1], K.transpose(-2, -1)) / (self.d_k ** 0.5)\n",
    "            else:  # Pencere bazlı dikkat\n",
    "                start = max(0, i - self.window_size)\n",
    "                end = min(seq_len, i + self.window_size + 1)\n",
    "                scores[:, :, i, start:end] = torch.matmul(Q[:, :, i:i+1], K[:, :, start:end].transpose(-2, -1)) / (self.d_k ** 0.5)\n",
    "        \n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        context = torch.matmul(attn, V)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        return self.W_o(context)\n",
    "\n",
    "# State Space Model (SSM)\n",
    "class SimpleSSM(nn.Module):\n",
    "    def __init__(self, d_model, state_dim=16, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.state_dim = state_dim\n",
    "        self.A = nn.Parameter(torch.randn(state_dim, state_dim))\n",
    "        self.B = nn.Linear(d_model, state_dim)\n",
    "        self.C = nn.Linear(state_dim, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        state = torch.zeros(batch_size, self.state_dim, device=x.device)\n",
    "        outputs = []\n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            state = state @ self.A + self.B(x[:, t])\n",
    "            output = self.C(state)\n",
    "            outputs.append(output)\n",
    "        \n",
    "        return torch.stack(outputs, dim=1)\n",
    "\n",
    "# Sınıflandırma modeli\n",
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, layers_config, latent_dim=None):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(30522, d_model)\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        for layer_type in layers_config:\n",
    "            if layer_type == \"MHA\":\n",
    "                self.layers.append(MultiHeadAttention(d_model, num_heads, use_rope=True))\n",
    "            elif layer_type == \"MLA\":\n",
    "                self.layers.append(MultiHeadLatentAttention(d_model, num_heads, latent_dim, use_rope=True))\n",
    "            elif layer_type == \"Longformer\":\n",
    "                self.layers.append(LongformerAttention(d_model, num_heads))\n",
    "            elif layer_type == \"SSM\":\n",
    "                self.layers.append(SimpleSSM(d_model))\n",
    "        \n",
    "        self.fc = nn.Linear(d_model, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        cls_output = x[:, 0]  # CLS token\n",
    "        return self.fc(cls_output)\n",
    "\n",
    "# Parametre sayısını hesaplama\n",
    "def count_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total_params, trainable_params\n",
    "\n",
    "# Veri setini hazırlama (Daha büyük veri seti)\n",
    "def prepare_data(batch_size=32, seq_len=128):\n",
    "    dataset = load_dataset(\"imdb\", split=\"train[:5000]\")  # Daha büyük veri seti\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        [\"[CLS] \" + text for text in dataset[\"text\"]], \n",
    "        max_length=seq_len, \n",
    "        padding=\"max_length\", \n",
    "        truncation=True, \n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    labels = torch.tensor(dataset[\"label\"])\n",
    "    \n",
    "    train_size = 4000\n",
    "    val_size = 500\n",
    "    test_size = 500\n",
    "    \n",
    "    train_data = input_ids[:train_size]\n",
    "    train_labels = labels[:train_size]\n",
    "    val_data = input_ids[train_size:train_size+val_size]\n",
    "    val_labels = labels[train_size:train_size+val_size]\n",
    "    test_data = input_ids[train_size+val_size:]\n",
    "    test_labels = labels[train_size+val_size:]\n",
    "    \n",
    "    num_train_batches = train_size // batch_size\n",
    "    train_data = train_data[:num_train_batches * batch_size].view(num_train_batches, batch_size, seq_len)\n",
    "    train_labels = train_labels[:num_train_batches * batch_size].view(num_train_batches, batch_size)\n",
    "    \n",
    "    num_val_batches = val_size // batch_size\n",
    "    val_data = val_data[:num_val_batches * batch_size].view(num_val_batches, batch_size, seq_len)\n",
    "    val_labels = val_labels[:num_val_batches * batch_size].view(num_val_batches, batch_size)\n",
    "    \n",
    "    num_test_batches = test_size // batch_size\n",
    "    test_data = test_data[:num_test_batches * batch_size].view(num_test_batches, batch_size, seq_len)\n",
    "    test_labels = test_labels[:num_test_batches * batch_size].view(num_test_batches, batch_size)\n",
    "    \n",
    "    print(f\"Eğitim veri boyutu: {train_data.shape}\")\n",
    "    print(f\"Validasyon veri boyutu: {val_data.shape}\")\n",
    "    print(f\"Test veri boyutu: {test_data.shape}\")\n",
    "    return (train_data, train_labels), (val_data, val_labels), (test_data, test_labels)\n",
    "\n",
    "# Modeli eğitme\n",
    "def train_model(model, train_data, train_labels, val_data, val_labels, epochs=5, lr=0.0001):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    results = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": [], \"time\": [], \"memory\": []}\n",
    "    \n",
    "    for epoch in tqdm(range(epochs), desc=\"Epochs\"):\n",
    "        model.train()\n",
    "        epoch_train_loss = 0\n",
    "        epoch_train_correct = 0\n",
    "        epoch_time = 0\n",
    "        epoch_memory = 0\n",
    "        total_train_samples = 0\n",
    "        \n",
    "        print(f\"\\n=== Epoch {epoch+1}/{epochs} - Eğitim ===\")\n",
    "        for i, (batch_data, batch_labels) in enumerate(zip(train_data, train_labels)):\n",
    "            tracemalloc.start()\n",
    "            start_time = time.time()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_data)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            end_time = time.time()\n",
    "            _, peak_memory = tracemalloc.get_traced_memory()\n",
    "            tracemalloc.stop()\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct = (predicted == batch_labels).sum().item()\n",
    "            batch_size = batch_labels.size(0)\n",
    "            \n",
    "            epoch_train_loss += loss.item()\n",
    "            epoch_train_correct += correct\n",
    "            epoch_time += end_time - start_time\n",
    "            epoch_memory += peak_memory / 1024 / 1024\n",
    "            total_train_samples += batch_size\n",
    "            \n",
    "            print(f\"Batch {i+1}/{len(train_data)} - Kayıp: {loss.item():.4f} - Doğruluk: {correct/batch_size:.4f}\")\n",
    "        \n",
    "        model.eval()\n",
    "        epoch_val_loss = 0\n",
    "        epoch_val_correct = 0\n",
    "        total_val_samples = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_data, batch_labels in zip(val_data, val_labels):\n",
    "                outputs = model(batch_data)\n",
    "                loss = criterion(outputs, batch_labels)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct = (predicted == batch_labels).sum().item()\n",
    "                batch_size = batch_labels.size(0)\n",
    "                \n",
    "                epoch_val_loss += loss.item()\n",
    "                epoch_val_correct += correct\n",
    "                total_val_samples += batch_size\n",
    "        \n",
    "        train_loss = epoch_train_loss / len(train_data)\n",
    "        train_acc = epoch_train_correct / total_train_samples\n",
    "        val_loss = epoch_val_loss / len(val_data)\n",
    "        val_acc = epoch_val_correct / total_val_samples\n",
    "        avg_time = epoch_time / len(train_data)\n",
    "        avg_memory = epoch_memory / len(train_data)\n",
    "        \n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"val_loss\"].append(val_loss)\n",
    "        results[\"val_acc\"].append(val_acc)\n",
    "        results[\"time\"].append(avg_time)\n",
    "        results[\"memory\"].append(avg_memory)\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs} Özeti:\")\n",
    "        print(f\" - Eğitim Kayıp: {train_loss:.4f} - Eğitim Doğruluk: {train_acc:.4f}\")\n",
    "        print(f\" - Validasyon Kayıp: {val_loss:.4f} - Validasyon Doğruluk: {val_acc:.4f}\")\n",
    "        print(f\" - Ortalama Batch Süresi: {avg_time:.4f} saniye\")\n",
    "        print(f\" - Ortalama Bellek Kullanımı: {avg_memory:.2f} MB\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test etme\n",
    "def test_model(model, test_data, test_labels):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_loss = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (batch_data, batch_labels) in enumerate(zip(test_data, test_labels)):\n",
    "            outputs = model(batch_data)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += batch_labels.size(0)\n",
    "            correct += (predicted == batch_labels).sum().item()\n",
    "            test_loss += loss.item()\n",
    "            print(f\"Test Batch {i+1}/{len(test_data)} - Kayıp: {loss.item():.4f} - Doğruluk: {(predicted == batch_labels).sum().item()/batch_labels.size(0):.4f}\")\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    avg_loss = test_loss / len(test_data)\n",
    "    print(f\"\\nTest Özeti:\")\n",
    "    print(f\" - Ortalama Kayıp: {avg_loss:.4f}\")\n",
    "    print(f\" - Doğruluk: {accuracy:.4f}\")\n",
    "    print(f\" - Toplam Örnek Sayısı: {total}\")\n",
    "    return accuracy, avg_loss\n",
    "\n",
    "# Ana fonksiyon\n",
    "def main():\n",
    "    batch_size, seq_len, d_model, num_heads = 32, 128, 512, 8\n",
    "    epochs = 1\n",
    "    lr = 0.0001\n",
    "    latent_dims = [64, 128]\n",
    "    \n",
    "    print(\"Veri seti hazırlanıyor...\")\n",
    "    (train_data, train_labels), (val_data, val_labels), (test_data, test_labels) = prepare_data(batch_size, seq_len)\n",
    "    \n",
    "    print(\"\\n=== Hiperparametreler ===\")\n",
    "    print(f\"Batch Size: {batch_size}\")\n",
    "    print(f\"Sequence Length: {seq_len}\")\n",
    "    print(f\"Model Dimension: {d_model}\")\n",
    "    print(f\"Number of Heads: {num_heads}\")\n",
    "    print(f\"Epochs: {epochs}\")\n",
    "    print(f\"Learning Rate: {lr}\")\n",
    "    \n",
    "    models = {\n",
    "        \"MHA\": SentimentClassifier(d_model, num_heads, [\"MHA\", \"MHA\"]),\n",
    "        \"MLA_64\": SentimentClassifier(d_model, num_heads, [\"MLA\", \"MHA\"], latent_dim=64),\n",
    "        \"MLA_128\": SentimentClassifier(d_model, num_heads, [\"MLA\", \"MHA\"], latent_dim=128),\n",
    "        \"Longformer\": SentimentClassifier(d_model, num_heads, [\"Longformer\", \"MHA\"]),\n",
    "        \"SSM_Hybrid\": SentimentClassifier(d_model, num_heads, [\"SSM\", \"MHA\"])\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        print(f\"\\n=== {name} Modeli ===\")\n",
    "        total_params, trainable_params = count_parameters(model)\n",
    "        print(f\"Toplam Parametre Sayısı: {total_params:,}\")\n",
    "        print(f\"Eğitilebilir Parametre Sayısı: {trainable_params:,}\")\n",
    "        \n",
    "        print(f\"\\n{name} modeli eğitiliyor...\")\n",
    "        results[name] = train_model(model, train_data, train_labels, val_data, val_labels, epochs, lr)\n",
    "        accuracy, test_loss = test_model(model, test_data, test_labels)\n",
    "        results[name][\"test_acc\"] = accuracy\n",
    "        results[name][\"test_loss\"] = test_loss\n",
    "    \n",
    "    # Görselleştirme\n",
    "    epochs_range = range(1, epochs + 1)\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    \n",
    "    for i, (metric, title) in enumerate([\n",
    "        (\"train_loss\", \"Eğitim Kaybı\"), (\"train_acc\", \"Eğitim Doğruluğu\"),\n",
    "        (\"val_loss\", \"Validasyon Kaybı\"), (\"val_acc\", \"Validasyon Doğruluğu\"),\n",
    "        (\"time\", \"Ortalama Batch Süresi (s)\"), (\"memory\", \"Ortalama Bellek Kullanımı (MB)\")\n",
    "    ], 1):\n",
    "        plt.subplot(3, 2, i)\n",
    "        for name in models.keys():\n",
    "            plt.plot(epochs_range, results[name][metric], label=name)\n",
    "        plt.title(title)\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(title.split()[-1])\n",
    "        plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Yorumlar ve Öneriler\n",
    "    print(\"\\n=== Yorumlar ve Öneriler ===\")\n",
    "    print(\"1. Performans: SSM ve Longformer, uzun dizilerde avantaj sağlayabilir. Şu an 5000 örnek kullanıldı, farklar daha belirgin hale geldi.\")\n",
    "    print(\"   Daha büyük bir veri seti (örneğin tüm IMDB) ile daha iyi sonuçlar alınabilir.\")\n",
    "    print(\"2. Karmaşıklık: Katman yığma ve SSM eklenmesi parametre sayısını artırdı, dropout (0.1) ile overfitting önlenmeye çalışıldı.\")\n",
    "    print(\"   Daha fazla regularization (örneğin weight decay) düşünülebilir.\")\n",
    "    print(\"3. İyileştirme: LongformerAttention, CLS token’a global dikkat ile geliştirildi. Daha gerçekçi bir Longformer için dilated attention eklenebilir.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import time\n",
    "import tracemalloc\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# RoPE için yardımcı fonksiyon\n",
    "def apply_rotary_pos_emb(q, k, seq_len, dim):\n",
    "    theta = torch.arange(0, dim, 2, dtype=torch.float, device=q.device) / dim\n",
    "    theta = 10000 ** (-theta)\n",
    "    positions = torch.arange(seq_len, dtype=torch.float, device=q.device).unsqueeze(1)\n",
    "    angles = positions * theta.unsqueeze(0)\n",
    "    sin_angles = torch.sin(angles).unsqueeze(0).unsqueeze(0)\n",
    "    cos_angles = torch.cos(angles).unsqueeze(0).unsqueeze(0)\n",
    "    \n",
    "    q_reshape = q.reshape(*q.shape[:-1], -1, 2)\n",
    "    k_reshape = k.reshape(*k.shape[:-1], -1, 2)\n",
    "    \n",
    "    q_rot = torch.cat([\n",
    "        q_reshape[..., 0] * cos_angles - q_reshape[..., 1] * sin_angles,\n",
    "        q_reshape[..., 0] * sin_angles + q_reshape[..., 1] * cos_angles\n",
    "    ], dim=-1)\n",
    "    k_rot = torch.cat([\n",
    "        k_reshape[..., 0] * cos_angles - k_reshape[..., 1] * sin_angles,\n",
    "        k_reshape[..., 0] * sin_angles + k_reshape[..., 1] * cos_angles\n",
    "    ], dim=-1)\n",
    "    \n",
    "    return q_rot.view_as(q), k_rot.view_as(k)\n",
    "\n",
    "# Multi-head Latent Attention (MLA)\n",
    "class MultiHeadLatentAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, latent_dim, dropout=0.1):\n",
    "        super(MultiHeadLatentAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_kv_down = nn.Linear(d_model, latent_dim)\n",
    "        self.W_k_up = nn.Linear(latent_dim, d_model)\n",
    "        self.W_v_up = nn.Linear(latent_dim, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        x = self.norm(x)\n",
    "        Q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        kv_latent = self.W_kv_down(x)\n",
    "        K = self.W_k_up(kv_latent).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v_up(kv_latent).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        Q, K = apply_rotary_pos_emb(Q, K, seq_len, self.d_k)\n",
    "        \n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_k ** 0.5)\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        context = torch.matmul(attn, V)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        return self.W_o(context) + x  # Residual connection\n",
    "\n",
    "# State Space Model (SSM)\n",
    "class SimpleSSM(nn.Module):\n",
    "    def __init__(self, d_model, state_dim=16, dropout=0.1):\n",
    "        super(SimpleSSM, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.state_dim = state_dim\n",
    "        self.A = nn.Parameter(torch.randn(state_dim, state_dim) * 0.01)\n",
    "        self.B = nn.Linear(d_model, state_dim)\n",
    "        self.C = nn.Linear(state_dim, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        x = self.norm(x)\n",
    "        state = torch.zeros(batch_size, self.state_dim, device=x.device)\n",
    "        outputs = []\n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            state = state @ self.A + self.B(x[:, t])\n",
    "            output = self.C(state)\n",
    "            outputs.append(output)\n",
    "        \n",
    "        output = torch.stack(outputs, dim=1)\n",
    "        return self.dropout(output) + x  # Residual connection\n",
    "\n",
    "# LLM Modeli\n",
    "class CustomLLM(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, latent_dim, num_layers=2, dropout=0.1):\n",
    "        super(CustomLLM, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "        # Düzgün bir liste oluştur\n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            if i % 2 == 0:\n",
    "                layers.append(MultiHeadLatentAttention(d_model, num_heads, latent_dim, dropout))\n",
    "            else:\n",
    "                # Fixed: Corrected the parameter order\n",
    "                layers.append(SimpleSSM(d_model, state_dim=latent_dim, dropout=dropout))\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        \n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.fc = nn.Linear(d_model, 2)  # Duygu analizi için 2 sınıf\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = self.norm(x)\n",
    "        cls_output = x[:, 0]  # CLS token\n",
    "        return self.fc(cls_output)\n",
    "\n",
    "# Parametre sayısını hesaplama\n",
    "def count_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total_params, trainable_params\n",
    "\n",
    "# Veri setini hazırlama\n",
    "def prepare_data(batch_size=32, seq_len=128):\n",
    "    dataset = load_dataset(\"imdb\", split=\"train[:5000]\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        [\"[CLS] \" + text for text in dataset[\"text\"]], \n",
    "        max_length=seq_len, \n",
    "        padding=\"max_length\", \n",
    "        truncation=True, \n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    labels = torch.tensor(dataset[\"label\"])\n",
    "    \n",
    "    train_size = 4000\n",
    "    val_size = 500\n",
    "    test_size = 500\n",
    "    \n",
    "    train_data = input_ids[:train_size]\n",
    "    train_labels = labels[:train_size]\n",
    "    val_data = input_ids[train_size:train_size + val_size]\n",
    "    val_labels = labels[train_size:train_size + val_size]\n",
    "    test_data = input_ids[train_size + val_size:]\n",
    "    test_labels = labels[train_size + val_size:]\n",
    "    \n",
    "    num_train_batches = train_size // batch_size\n",
    "    train_data = train_data[:num_train_batches * batch_size].view(num_train_batches, batch_size, seq_len)\n",
    "    train_labels = train_labels[:num_train_batches * batch_size].view(num_train_batches, batch_size)\n",
    "    \n",
    "    num_val_batches = val_size // batch_size\n",
    "    val_data = val_data[:num_val_batches * batch_size].view(num_val_batches, batch_size, seq_len)\n",
    "    val_labels = val_labels[:num_val_batches * batch_size].view(num_val_batches, batch_size)\n",
    "    \n",
    "    num_test_batches = test_size // batch_size\n",
    "    test_data = test_data[:num_test_batches * batch_size].view(num_test_batches, batch_size, seq_len)\n",
    "    test_labels = test_labels[:num_test_batches * batch_size].view(num_test_batches, batch_size)\n",
    "    test_texts = dataset[\"text\"][train_size + val_size:train_size + val_size + num_test_batches * batch_size]\n",
    "    \n",
    "    # Veri seti dağılımını kontrol et\n",
    "    print(f\"Eğitim veri boyutu: {train_data.shape}, Etiket Dağılımı: {torch.bincount(train_labels.flatten())}\")\n",
    "    print(f\"Validasyon veri boyutu: {val_data.shape}, Etiket Dağılımı: {torch.bincount(val_labels.flatten())}\")\n",
    "    print(f\"Test veri boyutu: {test_data.shape}, Etiket Dağılımı: {torch.bincount(test_labels.flatten())}\")\n",
    "    return (train_data, train_labels), (val_data, val_labels), (test_data, test_labels, test_texts), tokenizer.vocab_size, tokenizer\n",
    "\n",
    "# Modeli eğitme\n",
    "def train_model(model, train_data, train_labels, val_data, val_labels, epochs=5, lr=0.0001):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    results = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": [], \"time\": [], \"memory\": []}\n",
    "    \n",
    "    for epoch in tqdm(range(epochs), desc=\"Epochs\"):\n",
    "        model.train()\n",
    "        epoch_train_loss = 0\n",
    "        epoch_train_correct = 0\n",
    "        epoch_time = 0\n",
    "        epoch_memory = 0\n",
    "        total_train_samples = 0\n",
    "        \n",
    "        print(f\"\\n=== Epoch {epoch + 1}/{epochs} - Eğitim ===\")\n",
    "        for i, (batch_data, batch_labels) in enumerate(zip(train_data, train_labels)):\n",
    "            tracemalloc.start()\n",
    "            start_time = time.time()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_data)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            end_time = time.time()\n",
    "            _, peak_memory = tracemalloc.get_traced_memory()\n",
    "            tracemalloc.stop()\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct = (predicted == batch_labels).sum().item()\n",
    "            batch_size = batch_labels.size(0)\n",
    "            \n",
    "            epoch_train_loss += loss.item()\n",
    "            epoch_train_correct += correct\n",
    "            epoch_time += end_time - start_time\n",
    "            epoch_memory += peak_memory / 1024 / 1024\n",
    "            total_train_samples += batch_size\n",
    "            \n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"Batch {i + 1}/{len(train_data)} - Kayıp: {loss.item():.4f} - Doğruluk: {correct / batch_size:.4f}\")\n",
    "        \n",
    "        model.eval()\n",
    "        epoch_val_loss = 0\n",
    "        epoch_val_correct = 0\n",
    "        total_val_samples = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_data, batch_labels in zip(val_data, val_labels):\n",
    "                outputs = model(batch_data)\n",
    "                loss = criterion(outputs, batch_labels)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct = (predicted == batch_labels).sum().item()\n",
    "                batch_size = batch_labels.size(0)\n",
    "                \n",
    "                epoch_val_loss += loss.item()\n",
    "                epoch_val_correct += correct\n",
    "                total_val_samples += batch_size\n",
    "        \n",
    "        train_loss = epoch_train_loss / len(train_data)\n",
    "        train_acc = epoch_train_correct / total_train_samples\n",
    "        val_loss = epoch_val_loss / len(val_data)\n",
    "        val_acc = epoch_val_correct / total_val_samples\n",
    "        avg_time = epoch_time / len(train_data)\n",
    "        avg_memory = epoch_memory / len(train_data)\n",
    "        \n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"val_loss\"].append(val_loss)\n",
    "        results[\"val_acc\"].append(val_acc)\n",
    "        results[\"time\"].append(avg_time)\n",
    "        results[\"memory\"].append(avg_memory)\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch + 1}/{epochs} Özeti:\")\n",
    "        print(f\" - Eğitim Kayıp: {train_loss:.4f} - Eğitim Doğruluk: {train_acc:.4f}\")\n",
    "        print(f\" - Validasyon Kayıp: {val_loss:.4f} - Validasyon Doğruluk: {val_acc:.4f}\")\n",
    "        print(f\" - Ortalama Batch Süresi: {avg_time:.4f} saniye\")\n",
    "        print(f\" - Ortalama Bellek Kullanımı: {avg_memory:.2f} MB\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test etme\n",
    "def test_model(model, test_data, test_labels, test_texts, tokenizer, num_samples=5):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_loss = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_outputs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (batch_data, batch_labels) in enumerate(zip(test_data, test_labels)):\n",
    "            outputs = model(batch_data)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            softmax_outputs = torch.softmax(outputs, dim=1)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += batch_labels.size(0)\n",
    "            correct += (predicted == batch_labels).sum().item()\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(batch_labels.cpu().numpy())\n",
    "            all_outputs.extend(softmax_outputs.cpu().numpy())\n",
    "            \n",
    "            print(f\"Test Batch {i + 1}/{len(test_data)} - Kayıp: {loss.item():.4f} - Doğruluk: {(predicted == batch_labels).sum().item() / batch_labels.size(0):.4f}\")\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    avg_loss = test_loss / len(test_data)\n",
    "    \n",
    "    print(f\"\\nTest Özeti:\")\n",
    "    print(f\" - Ortalama Kayıp: {avg_loss:.4f}\")\n",
    "    print(f\" - Doğruluk: {accuracy:.4f}\")\n",
    "    print(f\" - Toplam Örnek Sayısı: {total}\")\n",
    "    print(f\" - Tahmin Dağılımı: {np.bincount(all_preds)}\")\n",
    "    print(f\" - Gerçek Etiket Dağılımı: {np.bincount(all_labels)}\")\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    if len(np.unique(all_labels)) < 2 or len(np.unique(all_preds)) < 2:\n",
    "        print(\"Uyarı: Test verisinde tek bir sınıf tahmin edildi veya mevcut, Confusion Matrix tam anlamıyla çizilemez.\")\n",
    "        print(f\"Confusion Matrix: {cm}\")\n",
    "    else:\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Negatif\", \"Pozitif\"])\n",
    "        fig, ax = plt.subplots()\n",
    "        disp.plot(cmap=plt.cm.Blues, ax=ax)\n",
    "        ax.set_xticks([0, 1])\n",
    "        ax.set_yticks([0, 1])\n",
    "        ax.set_xticklabels([\"Negatif\", \"Pozitif\"])\n",
    "        ax.set_yticklabels([\"Negatif\", \"Pozitif\"])\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "        plt.show()\n",
    "    \n",
    "    # Örnek Tahminler\n",
    "    print(f\"\\n=== Örnek Tahminler (İlk {num_samples} Örnek) ===\")\n",
    "    for i in range(min(num_samples, len(test_texts))):\n",
    "        text = test_texts[i][:100] + \"...\" if len(test_texts[i]) > 100 else test_texts[i]\n",
    "        true_label = \"Pozitif\" if all_labels[i] == 1 else \"Negatif\"\n",
    "        pred_label = \"Pozitif\" if all_preds[i] == 1 else \"Negatif\"\n",
    "        softmax_scores = all_outputs[i]\n",
    "        print(f\"Metin: {text}\")\n",
    "        print(f\"Gerçek Etiket: {true_label} - Tahmin Edilen Etiket: {pred_label}\")\n",
    "        print(f\"Softmax Skorları: Negatif: {softmax_scores[0]:.4f}, Pozitif: {softmax_scores[1]:.4f}\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    return accuracy, avg_loss\n",
    "\n",
    "# Ana fonksiyon\n",
    "def main():\n",
    "    batch_size, seq_len, d_model, num_heads = 32, 128, 512, 8\n",
    "    epochs = 2  # Daha fazla epoch için artırıldı\n",
    "    lr = 0.0001\n",
    "    latent_dim = 128\n",
    "    num_layers = 4\n",
    "    \n",
    "    print(\"Veri seti hazırlanıyor...\")\n",
    "    (train_data, train_labels), (val_data, val_labels), (test_data, test_labels, test_texts), vocab_size, tokenizer = prepare_data(batch_size, seq_len)\n",
    "    \n",
    "    print(\"\\n=== Hiperparametreler ===\")\n",
    "    print(f\"Batch Size: {batch_size}\")\n",
    "    print(f\"Sequence Length: {seq_len}\")\n",
    "    print(f\"Model Dimension: {d_model}\")\n",
    "    print(f\"Number of Heads: {num_heads}\")\n",
    "    print(f\"Latent Dimension: {latent_dim}\")\n",
    "    print(f\"Number of Layers: {num_layers}\")\n",
    "    print(f\"Epochs: {epochs}\")\n",
    "    print(f\"Learning Rate: {lr}\")\n",
    "    print(f\"Vocabulary Size: {vocab_size}\")\n",
    "    \n",
    "    # Modeli oluştur\n",
    "    model = CustomLLM(vocab_size, d_model, num_heads, latent_dim, num_layers)\n",
    "    total_params, trainable_params = count_parameters(model)\n",
    "    print(f\"\\n=== CustomLLM Modeli ===\")\n",
    "    print(f\"Toplam Parametre Sayısı: {total_params:,}\")\n",
    "    print(f\"Eğitilebilir Parametre Sayısı: {trainable_params:,}\")\n",
    "    \n",
    "    print(\"\\nModel eğitiliyor...\")\n",
    "    results = train_model(model, train_data, train_labels, val_data, val_labels, epochs, lr)\n",
    "    accuracy, test_loss = test_model(model, test_data, test_labels, test_texts, tokenizer)\n",
    "    results[\"test_acc\"] = accuracy\n",
    "    results[\"test_loss\"] = test_loss\n",
    "    \n",
    "    # Görselleştirme\n",
    "    epochs_range = range(1, epochs + 1)\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    \n",
    "    for i, (metric, title) in enumerate([\n",
    "        (\"train_loss\", \"Eğitim Kaybı\"), (\"train_acc\", \"Eğitim Doğruluğu\"),\n",
    "        (\"val_loss\", \"Validasyon Kaybı\"), (\"val_acc\", \"Validasyon Doğruluğu\"),\n",
    "        (\"time\", \"Ortalama Batch Süresi (s)\"), (\"memory\", \"Ortalama Bellek Kullanımı (MB)\")\n",
    "    ], 1):\n",
    "        plt.subplot(3, 2, i)\n",
    "        plt.plot(epochs_range, results[metric], label=\"CustomLLM\")\n",
    "        plt.title(title)\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(title.split()[-1])\n",
    "        plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Yorumlar ve Öneriler\n",
    "    print(\"\\n=== Yorumlar ve Öneriler ===\")\n",
    "    print(\"1. Performans: Model, test setinde yalnızca Negatif tahmin etti. Veri seti dağılımı kontrol edilmeli veya daha fazla epoch ile genelleme artırılmalı.\")\n",
    "    print(\"2. Karmaşıklık: Eğitim çok hızlı tamamlandı, öğrenme oranı (lr) artırılabilir (örneğin 0.001) veya model kapasitesi genişletilebilir.\")\n",
    "    print(\"3. İyileştirme: Softmax skorları incelenerek modelin sınıf偏見 (bias) durumu analiz edilmeli. Daha dengeli bir veri seti kullanılabilir.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
