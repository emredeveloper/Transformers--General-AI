{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RMSNorm, girdiyi normalize eder ve stabilite sağlar\n",
    "# Burada, girdinin elemanlarının karelerinin ortalamasını alıp karekökü ile bölüyoruz.\n",
    "#\n",
    "\n",
    "def normalize(x):\n",
    "    return x/ np.sqrt(np.sum(x**2) + 1e-6) # 1e-8, sıfıra bölmeyi önlemek için eklenir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.13483997, 0.26967994, 0.40451991, 0.53935989, 0.67419986])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize(np.array([1,2,3,4,5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Girdi:\n",
      " [[ 1.10517535 -1.1437946  -0.6105434   0.61707477 -1.25953267 -1.60542219\n",
      "  -1.12414634 -0.09881562]]\n",
      "MoE Çıktısı:\n",
      " [[ 0.3041326   0.3533763   0.05777129  0.5359059  -0.3078998  -0.07377402\n",
      "  -0.33160865  0.53093004]]\n"
     ]
    }
   ],
   "source": [
    "# Mixture of Experts\n",
    "class MixtureOfExperts:\n",
    "    def __init__(self, num_experts, hidden_dim):\n",
    "        # Uzmanların (experts) sayısını ve her birinin boyutunu belirtiyoruz.\n",
    "        self.num_experts = num_experts\n",
    "        # Her uzman için rastgele ağırlık matrisi oluşturuyoruz.\n",
    "        self.experts = [np.random.randn(hidden_dim, hidden_dim) for _ in range(num_experts)]\n",
    "\n",
    "    def route(self, x):\n",
    "        # Router (yönlendirici), hangi uzmanların kullanılacağını seçer.\n",
    "        # Basit bir örnek uygulaması: Uzmanlara rastgele skor veriyoruz.\n",
    "        scores = np.random.rand(self.num_experts)  # Her uzman için rastgele skor oluştur.\n",
    "        top_k_indices = np.argsort(scores)[-2:]  # En yüksek skora sahip 2 uzmanı seç.\n",
    "        return top_k_indices\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Router tarafından seçilen uzmanlardan geçiş yapıyoruz.\n",
    "        selected_experts = self.route(x)  # Seçilen uzmanların indekslerini alıyoruz.\n",
    "        output = 0  # Başlangıç değeri\n",
    "        for idx in selected_experts:\n",
    "            # Seçilen uzmanlardan sırasıyla geçiş yapıyoruz.\n",
    "            output += np.dot(x, self.experts[idx])  # Uzmanın ağırlık matrisi ile çarp.\n",
    "        return normalize(output)  # Sonucu normalize ederek döndür.\n",
    "\n",
    "# Örnek kullanım\n",
    "hidden_dim = 8\n",
    "x = np.random.randn(1, hidden_dim)  # Tek bir giriş örneği (1 satır, 8 boyut).\n",
    "moe = MixtureOfExperts(num_experts=4, hidden_dim=hidden_dim)  # 4 uzmanlı sistem oluşturuyoruz.\n",
    "output = moe.forward(x)  # Girdiyi MoE'den geçiriyoruz.\n",
    "\n",
    "print(\"Girdi:\\n\", x)\n",
    "print(\"MoE Çıktısı:\\n\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicRouter:\n",
    "    def __init__(self, num_experts, num_tokens, hidden_size):\n",
    "        self.num_experts = num_experts\n",
    "        self.num_tokens = num_tokens\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.weights = np.random.rand(num_experts, num_tokens) # Expertlerin ağırlıkları\n",
    "        self.bias = np.random.rand(num_experts) # Expertlerin biasları\n",
    "        \n",
    "    def route(x):\n",
    "        scores = np.dot(self.weights, x) + self.bias # Expertlerin skorları\n",
    "        scores = normalize(scores) # Skorları normalize ediyoruz.\n",
    "        topk_incdice = np.argsort(scores)[-self.hidden_size:]\n",
    "        return topk_incdice # En yüksek skorları döndürüyoruz.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basit bir veri kümesi (örnek metinler ve etiketler)\n",
    "data = [\n",
    "    (\"I love this product\", 1),\n",
    "    (\"This is amazing\", 1),\n",
    "    (\"Absolutely fantastic\", 1),\n",
    "    (\"I hate this\", 0),\n",
    "    (\"This is terrible\", 0),\n",
    "    (\"Absolutely awful\", 0),\n",
    "]\n",
    "\n",
    "# Metinleri vektörlere dönüştürmek için bir kelime dağarcığı\n",
    "vocab = {word: idx for idx, word in enumerate(set(\" \".join([d[0] for d in data]).split()))}\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Veriyi sayısal hale getiren bir fonksiyon\n",
    "def text_to_vector(text, vocab):\n",
    "    vector = [vocab[word] for word in text.split()]\n",
    "    return vector\n",
    "\n",
    "# PyTorch Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data, vocab):\n",
    "        self.data = [(torch.tensor(text_to_vector(text, vocab)), label) for text, label in data]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Collate fonksiyonu: Dataları aynı uzunluğa getirir.\n",
    "def collate_fn(batch):\n",
    "    texts, labels = zip(*batch)\n",
    "    padded_texts = pad_sequence(texts, batch_first=True, padding_value=0)  # Pad ile aynı uzunluğa getiriliyor.\n",
    "    labels = torch.tensor(labels)\n",
    "    return padded_texts, labels\n",
    "\n",
    "# DataLoader'da collate_fn kullanımı\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Veri kümesi ve DataLoader\n",
    "dataset = TextDataset(data, vocab)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.TextDataset at 0x2313b0f3e10>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixtureOfExperts(nn.Module):\n",
    "    def __init__(self, num_experts, embed_dim):\n",
    "        super().__init__()\n",
    "        self.num_experts = num_experts\n",
    "        self.experts = nn.ModuleList([nn.Linear(embed_dim, embed_dim) for _ in range(num_experts)])\n",
    "        self.gate = nn.Linear(embed_dim, num_experts)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Uzman skorlarını hesapla\n",
    "        gate_scores = torch.softmax(self.gate(x), dim=-1)  # [batch_size, num_experts]\n",
    "        \n",
    "        # Uzmanların çıktıları\n",
    "        expert_outputs = torch.stack([expert(x) for expert in self.experts], dim=-1)  # [batch_size, embed_dim, num_experts]\n",
    "        \n",
    "        # Kapılı uzman karışımı\n",
    "        output = torch.sum(expert_outputs * gate_scores.unsqueeze(1), dim=-1)  # [batch_size, embed_dim]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_hidden_dim):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_hidden_dim, embed_dim)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x, padding_mask=None):\n",
    "        # Self-Attention\n",
    "        attn_output, _ = self.attention(x, x, x, key_padding_mask=padding_mask)\n",
    "        x = self.norm1(x + attn_output)\n",
    "        \n",
    "        # Feed-Forward\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + ff_output)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(input_tensor, padding_value=0):\n",
    "    # Girdi tensöründe doldurma yapılan yerleri (0 değerini) bulur.\n",
    "    return (input_tensor == padding_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerWithMoE(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, ff_hidden_dim, num_experts):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.transformer = TransformerBlock(embed_dim, num_heads, ff_hidden_dim)\n",
    "        self.moe = MixtureOfExperts(num_experts, embed_dim)\n",
    "        self.classifier = nn.Linear(embed_dim, 2)  # İki sınıf (pozitif ve negatif)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Embedding\n",
    "        x = self.embedding(x)  # [batch_size, seq_len, embed_dim]\n",
    "        x = x.permute(1, 0, 2)  # Transformer için [seq_len, batch_size, embed_dim]\n",
    "        \n",
    "        # Padding maskesi\n",
    "        padding_mask = create_padding_mask(x.permute(1, 0, 2)[:, :, 0])  # [batch_size, seq_len]\n",
    "        \n",
    "        # Transformer Bloğu\n",
    "        x = self.transformer(x, padding_mask=padding_mask)\n",
    "        x = x.permute(1, 0, 2)  # [batch_size, seq_len, embed_dim]\n",
    "        \n",
    "        # MoE\n",
    "        x = self.moe(x.mean(dim=1))  # Sekansın ortalaması alınır (basitlik için)\n",
    "        \n",
    "        # Sınıflandırma\n",
    "        logits = self.classifier(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerWithMoE(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, ff_hidden_dim, num_experts):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)  # vocab_size, sözlük boyutuyla eşleşmeli\n",
    "        self.transformer = TransformerBlock(embed_dim, num_heads, ff_hidden_dim)\n",
    "        self.moe = MixtureOfExperts(num_experts, embed_dim)\n",
    "        self.classifier = nn.Linear(embed_dim, 2)  # İki sınıf (pozitif ve negatif)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Embedding\n",
    "        x = self.embedding(x)  # [batch_size, seq_len, embed_dim]\n",
    "        x = x.permute(1, 0, 2)  # Transformer için [seq_len, batch_size, embed_dim]\n",
    "        \n",
    "        # Transformer Bloğu\n",
    "        x = self.transformer(x)\n",
    "        x = x.permute(1, 0, 2)  # [batch_size, seq_len, embed_dim]\n",
    "        \n",
    "        # Mixture of Experts (MoE)\n",
    "        x = self.moe(x.mean(dim=1))  # Sekansın ortalaması alınır (basitlik için)\n",
    "        \n",
    "        # Sınıflandırma\n",
    "        logits = self.classifier(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.1026\n",
      "Epoch 2, Loss: 2.0711\n",
      "Epoch 3, Loss: 2.0545\n",
      "Epoch 4, Loss: 2.0393\n",
      "Epoch 5, Loss: 2.0245\n",
      "Epoch 6, Loss: 2.0096\n",
      "Epoch 7, Loss: 1.9945\n",
      "Epoch 8, Loss: 1.9789\n",
      "Epoch 9, Loss: 1.9626\n",
      "Epoch 10, Loss: 1.9452\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Hiperparametreler\n",
    "embed_dim = 16\n",
    "num_heads = 2\n",
    "ff_hidden_dim = 32\n",
    "num_experts = 4\n",
    "num_epochs = 10\n",
    "batch_size = 2\n",
    "\n",
    "# Model, Loss ve Optimizasyon\n",
    "model = TransformerWithMoE(vocab_size, embed_dim, num_heads, ff_hidden_dim, num_experts)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Custom collate function to pad sequences\n",
    "def collate_fn(batch):\n",
    "    inputs, labels = zip(*batch)\n",
    "    inputs = pad_sequence(inputs, batch_first=True)\n",
    "    labels = torch.tensor(labels)\n",
    "    return inputs, labels\n",
    "\n",
    "# Update your dataloader to use the custom collate function\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
    "\n",
    "# Eğitim döngüsü\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, labels in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'absolutely'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[93], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m vocab[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<UNK>\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(vocab)  \u001b[38;5;66;03m# <UNK> anahtarını sözlüğe ekliyoruz\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Metinleri vektörlere dönüştür ve padding işlemi yap\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m test_vectors \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_to_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     21\u001b[0m padded_test_vectors \u001b[38;5;241m=\u001b[39m pad_sequence(test_vectors, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Pad ile aynı uzunlukta yap\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(padded_test_vectors)\n",
      "Cell \u001b[1;32mIn[93], line 20\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     17\u001b[0m vocab[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<UNK>\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(vocab)  \u001b[38;5;66;03m# <UNK> anahtarını sözlüğe ekliyoruz\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Metinleri vektörlere dönüştür ve padding işlemi yap\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m test_vectors \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[43mtext_to_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m test_data]\n\u001b[0;32m     21\u001b[0m padded_test_vectors \u001b[38;5;241m=\u001b[39m pad_sequence(test_vectors, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Pad ile aynı uzunlukta yap\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(padded_test_vectors)\n",
      "Cell \u001b[1;32mIn[85], line 17\u001b[0m, in \u001b[0;36mtext_to_vector\u001b[1;34m(text, vocab)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtext_to_vector\u001b[39m(text, vocab):\n\u001b[1;32m---> 17\u001b[0m     vector \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m[\u001b[49m\u001b[43mword\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mword\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m vector\n",
      "Cell \u001b[1;32mIn[85], line 17\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtext_to_vector\u001b[39m(text, vocab):\n\u001b[1;32m---> 17\u001b[0m     vector \u001b[38;5;241m=\u001b[39m [\u001b[43mvocab\u001b[49m\u001b[43m[\u001b[49m\u001b[43mword\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m text\u001b[38;5;241m.\u001b[39msplit()]\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m vector\n",
      "\u001b[1;31mKeyError\u001b[0m: 'absolutely'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Test verileri\n",
    "test_data = [\n",
    "    \"I absolutely love this\",  # Pozitif\n",
    "    \"This is the worst\",       # Negatif\n",
    "    \"Fantastic product\",       # Pozitif\n",
    "    \"Not good at all\",         # Negatif\n",
    "    \"I am very happy\",         # Pozitif\n",
    "    \"Completely disappointed\"  # Negatif\n",
    "]\n",
    "\n",
    "# Metinleri vektörlere dönüştürmek için bir fonksiyon\n",
    "# Kelime dağarcığı (vocab) oluşturuluyor\n",
    "vocab = {word: idx for idx, word in enumerate(set(\" \".join([d[0] for d in data]).split()))}\n",
    "vocab['<UNK>'] = len(vocab)  # <UNK> anahtarını sözlüğe ekliyoruz\n",
    "\n",
    "# Metinleri vektörlere dönüştür ve padding işlemi yap\n",
    "test_vectors = [torch.tensor(text_to_vector(text, vocab)) for text in test_data]\n",
    "padded_test_vectors = pad_sequence(test_vectors, batch_first=True, padding_value=0)  # Pad ile aynı uzunlukta yap\n",
    "\n",
    "print(padded_test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kelime 'absolutely' sözlükte yok, '<UNK>' olarak işlenecek.\n",
      "Kelime 'the' sözlükte yok, '<UNK>' olarak işlenecek.\n",
      "Kelime 'worst' sözlükte yok, '<UNK>' olarak işlenecek.\n",
      "Kelime 'Fantastic' sözlükte yok, '<UNK>' olarak işlenecek.\n",
      "Kelime 'Not' sözlükte yok, '<UNK>' olarak işlenecek.\n",
      "Kelime 'good' sözlükte yok, '<UNK>' olarak işlenecek.\n",
      "Kelime 'at' sözlükte yok, '<UNK>' olarak işlenecek.\n",
      "Kelime 'all' sözlükte yok, '<UNK>' olarak işlenecek.\n",
      "Kelime 'am' sözlükte yok, '<UNK>' olarak işlenecek.\n",
      "Kelime 'very' sözlükte yok, '<UNK>' olarak işlenecek.\n",
      "Kelime 'happy' sözlükte yok, '<UNK>' olarak işlenecek.\n",
      "Kelime 'Completely' sözlükte yok, '<UNK>' olarak işlenecek.\n",
      "Kelime 'disappointed' sözlükte yok, '<UNK>' olarak işlenecek.\n",
      "Padded Test Vektörleri:\n",
      "tensor([[ 1, 12, 11,  4],\n",
      "        [ 5,  3, 12, 12],\n",
      "        [12,  2,  0,  0],\n",
      "        [12, 12, 12, 12],\n",
      "        [ 1, 12, 12, 12],\n",
      "        [12, 12,  0,  0]])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[78], line 65\u001b[0m\n\u001b[0;32m     62\u001b[0m predictions \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():  \u001b[38;5;66;03m# Modelin parametrelerini güncelleme\u001b[39;00m\n\u001b[1;32m---> 65\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpadded_test_vectors\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Tüm test girdilerini modele ver\u001b[39;00m\n\u001b[0;32m     66\u001b[0m     predicted_classes \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(outputs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Sınıf tahminlerini al\u001b[39;00m\n\u001b[0;32m     67\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m predicted_classes\u001b[38;5;241m.\u001b[39mtolist()  \u001b[38;5;66;03m# Liste formatına dönüştür\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\emreq\\Desktop\\Transformers\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\emreq\\Desktop\\Transformers\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[64], line 54\u001b[0m, in \u001b[0;36mTransformerWithMoE.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 54\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(x\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\emreq\\Desktop\\Transformers\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\emreq\\Desktop\\Transformers\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\emreq\\Desktop\\Transformers\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:190\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\emreq\\Desktop\\Transformers\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:2551\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2545\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2546\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2547\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2548\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2549\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2550\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Örnek eğitim verileri (sözlük oluşturmak için)\n",
    "data = [\n",
    "    (\"I love this product\", 1),\n",
    "    (\"This is amazing\", 1),\n",
    "    (\"Absolutely fantastic\", 1),\n",
    "    (\"I hate this\", 0),\n",
    "    (\"This is terrible\", 0),\n",
    "    (\"Absolutely awful\", 0),\n",
    "]\n",
    "\n",
    "# Kelime dağarcığı (vocab) oluşturuluyor\n",
    "vocab = {word: idx for idx, word in enumerate(set(\" \".join([d[0] for d in data]).split()))}\n",
    "vocab['<UNK>'] = len(vocab)  # <UNK> anahtarını ekliyoruz\n",
    "\n",
    "# Test verileri\n",
    "test_data = [\n",
    "    \"I absolutely love this\",\n",
    "    \"This is the worst\",\n",
    "    \"Fantastic product\",\n",
    "    \"Not good at all\",\n",
    "    \"I am very happy\",\n",
    "    \"Completely disappointed\"\n",
    "]\n",
    "\n",
    "# Test kelimelerini kontrol et\n",
    "for text in test_data:\n",
    "    for word in text.split():\n",
    "        if word not in vocab:\n",
    "            print(f\"Kelime '{word}' sözlükte yok, '<UNK>' olarak işlenecek.\")\n",
    "\n",
    "# Metinleri vektörlere dönüştürmek için bir fonksiyon\n",
    "def text_to_vector(text, vocab, unk_token='<UNK>'):\n",
    "    return [vocab.get(word, vocab[unk_token]) for word in text.split()]\n",
    "\n",
    "# Metinleri vektörlere dönüştür ve padding işlemi yap\n",
    "test_vectors = [torch.tensor(text_to_vector(text, vocab)) for text in test_data]\n",
    "\n",
    "# Sözlük boyutunu kontrol et\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Tüm indekslerin sınır içinde olup olmadığını kontrol et\n",
    "for vector in test_vectors:\n",
    "    if any(idx >= vocab_size for idx in vector):\n",
    "        print(f\"Hata! Bu vektör {vector} sözlük boyutunun dışına çıkıyor.\")\n",
    "\n",
    "# Padding işlemi yap\n",
    "padded_test_vectors = pad_sequence(test_vectors, batch_first=True, padding_value=0)\n",
    "\n",
    "# Padding sonrası kontrol et\n",
    "for vector in padded_test_vectors:\n",
    "    if any(idx >= vocab_size for idx in vector):\n",
    "        print(f\"Hata! Bu vektör {vector} sözlük boyutunun dışına çıkıyor.\")\n",
    "\n",
    "print(\"Padded Test Vektörleri:\")\n",
    "print(padded_test_vectors)\n",
    "\n",
    "# Modeli değerlendirme\n",
    "model.eval()  # Modeli değerlendirme moduna al\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():  # Modelin parametrelerini güncelleme\n",
    "    outputs = model(padded_test_vectors)  # Tüm test girdilerini modele ver\n",
    "    predicted_classes = torch.argmax(outputs, dim=-1)  # Sınıf tahminlerini al\n",
    "    predictions = predicted_classes.tolist()  # Liste formatına dönüştür\n",
    "\n",
    "# Sonuçları yazdır\n",
    "for text, prediction in zip(test_data, predictions):\n",
    "    result = \"Positive\" if prediction == 1 else \"Negative\"\n",
    "    print(f\"Text: \\\"{text}\\\" -> Prediction: {result}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
