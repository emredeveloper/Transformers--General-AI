{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def is_torch_greater_or_equal_than_1_13():\n",
    "    return torch.__version__ >= '1.13'\n",
    "\n",
    "# Test the function\n",
    "print(is_torch_greater_or_equal_than_1_13())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import warnings\n",
    "from typing import List, Optional, Tuple, Union\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint\n",
    "from torch import nn\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "from transformers.activations import ACT2FN\n",
    "from transformers.cache_utils import Cache, DynamicCache\n",
    "from transformers.modeling_attn_mask_utils import (\n",
    "    AttentionMaskConverter,\n",
    "    _prepare_4d_attention_mask,\n",
    "    _prepare_4d_causal_attention_mask,\n",
    "    _prepare_4d_causal_attention_mask_for_sdpa,\n",
    ")\n",
    "from transformers.modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast, SequenceClassifierOutputWithPast\n",
    "from transformers.modeling_utils import PreTrainedModel\n",
    "from transformers.utils import (\n",
    "    add_start_docstrings,\n",
    "    add_start_docstrings_to_model_forward,\n",
    "    is_flash_attn_2_available,\n",
    "    is_flash_attn_greater_or_equal_2_10,\n",
    "    logging,\n",
    "    replace_return_docstrings,\n",
    ")\n",
    "from transformers.utils.import_utils import is_torch_fx_available\n",
    "\n",
    "# Logging configuration\n",
    "logger = logging.get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom function to check torch version\n",
    "def is_torch_greater_or_equal_than_1_13():\n",
    "    return torch.__version__ >= '1.13'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import warnings\n",
    "from typing import List, Optional, Tuple, Union\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint\n",
    "from torch import nn\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "from transformers.activations import ACT2FN\n",
    "from transformers.cache_utils import Cache, DynamicCache\n",
    "from transformers.modeling_attn_mask_utils import (\n",
    "    AttentionMaskConverter,\n",
    "    _prepare_4d_attention_mask,\n",
    "    _prepare_4d_causal_attention_mask,\n",
    "    _prepare_4d_causal_attention_mask_for_sdpa,\n",
    ")\n",
    "from transformers.modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast, SequenceClassifierOutputWithPast\n",
    "from transformers.modeling_utils import PreTrainedModel\n",
    "from transformers.utils import (\n",
    "    add_start_docstrings,\n",
    "    add_start_docstrings_to_model_forward,\n",
    "    is_flash_attn_2_available,\n",
    "    is_flash_attn_greater_or_equal_2_10,\n",
    "    logging,\n",
    "    replace_return_docstrings,\n",
    ")\n",
    "from transformers.utils.import_utils import is_torch_fx_available\n",
    "\n",
    "# Logging configuration\n",
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "# Custom function to check torch version\n",
    "def is_torch_greater_or_equal_than_1_13():\n",
    "    return torch.__version__ >= '1.13'\n",
    "\n",
    "class SimpleTransformerModel(PreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        \n",
    "        self.embedding = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.transformer_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=config.hidden_size, \n",
    "            nhead=config.num_attention_heads, \n",
    "            dim_feedforward=4*config.hidden_size, \n",
    "            dropout=config.hidden_dropout_prob\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.transformer_layer, num_layers=config.num_hidden_layers)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "    ) -> Union[Tuple[torch.Tensor], SequenceClassifierOutputWithPast]:\n",
    "        # Embed the input ids\n",
    "        embeddings = self.embedding(input_ids)\n",
    "        \n",
    "        # Prepare attention mask if provided\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = _prepare_4d_attention_mask(attention_mask, embeddings.dtype)\n",
    "\n",
    "        # Pass through transformer encoder\n",
    "        transformer_output = self.transformer_encoder(embeddings.transpose(0, 1), src_key_padding_mask=(attention_mask == 0)).transpose(0, 1)\n",
    "        \n",
    "        # Pool the output by taking the first token (CLS token equivalent)\n",
    "        pooled_output = transformer_output[:, 0]\n",
    "        \n",
    "        # Pass through classifier\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.config.problem_type is None:\n",
    "                if self.config.num_labels == 1:\n",
    "                    self.config.problem_type = \"regression\"\n",
    "                elif self.config.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
    "                    self.config.problem_type = \"single_label_classification\"\n",
    "                else:\n",
    "                    self.config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "            if self.config.problem_type == \"regression\":\n",
    "                loss_fct = MSELoss()\n",
    "                if self.config.num_labels == 1:\n",
    "                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
    "                else:\n",
    "                    loss = loss_fct(logits, labels)\n",
    "            elif self.config.problem_type == \"single_label_classification\":\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.config.num_labels), labels.view(-1))\n",
    "            elif self.config.problem_type == \"multi_label_classification\":\n",
    "                loss_fct = BCEWithLogitsLoss()\n",
    "                loss = loss_fct(logits, labels)\n",
    "        \n",
    "        return SequenceClassifierOutputWithPast(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            past_key_values=None,  # Not used in this simple model\n",
    "        )\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you have a configuration object `config` with necessary parameters like vocab_size, hidden_size, etc.\n",
    "# model = SimpleTransformerModel(config)\n",
    "# output = model(input_ids, attention_mask=attention_mask, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizerFast, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from evaluate import load\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "# IMDb veri setini yükleme\n",
    "dataset = load_dataset('imdb')\n",
    "\n",
    "# Tokenizer seçimi\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize fonksiyonu\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "# Tokenized verileri hazırlama\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Gereksiz sütunları kaldırma\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['text'])\n",
    "\n",
    "# PyTorch tensör formatına dönüştürme\n",
    "tokenized_datasets.set_format('torch')\n",
    "\n",
    "# Eğitim ve test veri setlerini ayırma\n",
    "train_dataset = tokenized_datasets['train']\n",
    "test_dataset = tokenized_datasets['test']\n",
    "\n",
    "# Örnek veri kontrolü\n",
    "print(train_dataset[0])\n",
    "print(test_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=data_collator)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, collate_fn=data_collator)\n",
    "\n",
    "# Örnek veri kontrolü\n",
    "for batch in train_loader:\n",
    "    print(batch)\n",
    "    break\n",
    "\n",
    "for batch in test_loader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating language skills...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "SQuAD Evaluation: 100%|██████████| 10/10 [00:00<00:00, 19.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating coding skills...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Coding Evaluation: 100%|██████████| 10/10 [00:00<00:00, 11.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating conversation quality...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Conversation Evaluation: 100%|██████████| 10/10 [00:00<00:00, 23.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Evaluation Results:\n",
      "squad_accuracy: 0.8000\n",
      "sst2_accuracy: 0.4000\n",
      "code_accuracy: 0.2000\n",
      "conversation_score: 0.4821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    PretrainedConfig,\n",
    "    BertTokenizerFast,\n",
    "    BertModel,\n",
    "    BertPreTrainedModel,\n",
    "    AutoModelForQuestionAnswering,\n",
    "    pipeline,\n",
    "    modeling_outputs\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from typing import Optional, Union\n",
    "\n",
    "class SimpleTransformerConfig(PretrainedConfig):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size=30522,\n",
    "        hidden_size=768,\n",
    "        num_hidden_layers=6,\n",
    "        num_attention_heads=12,\n",
    "        intermediate_size=3072,\n",
    "        hidden_act=\"gelu\",\n",
    "        hidden_dropout_prob=0.1,\n",
    "        attention_probs_dropout_prob=0.1,\n",
    "        max_position_embeddings=512,\n",
    "        type_vocab_size=2,\n",
    "        initializer_range=0.02,\n",
    "        layer_norm_eps=1e-12,\n",
    "        pad_token_id=0,\n",
    "        position_embedding_type=\"absolute\",\n",
    "        use_cache=True,\n",
    "        classifier_dropout=None,\n",
    "        num_labels=2,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.hidden_act = hidden_act\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.hidden_dropout_prob = hidden_dropout_prob\n",
    "        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.type_vocab_size = type_vocab_size\n",
    "        self.initializer_range = initializer_range\n",
    "        self.layer_norm_eps = layer_norm_eps\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.position_embedding_type = position_embedding_type\n",
    "        self.use_cache = use_cache\n",
    "        self.classifier_dropout = classifier_dropout\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "class SimpleTransformerForSequenceClassification(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = torch.nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = torch.nn.Linear(config.hidden_size, config.num_labels)\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        token_type_ids: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[tuple, modeling_outputs.SequenceClassifierOutput]:\n",
    "        \n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = torch.nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.config.num_labels), labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return modeling_outputs.SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "\n",
    "def evaluate_language_skills(model, tokenizer):\n",
    "    print(\"Evaluating language skills...\")\n",
    "    results = {}\n",
    "    \n",
    "    try:\n",
    "        # SQuAD Evaluation\n",
    "        from transformers import DistilBertTokenizerFast\n",
    "        squad_sample = load_dataset('squad', split='validation').select(range(10))\n",
    "        \n",
    "        qa_tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "        qa_model = AutoModelForQuestionAnswering.from_pretrained('distilbert-base-uncased-distilled-squad')\n",
    "        \n",
    "        qa_pipeline = pipeline(\n",
    "            'question-answering',\n",
    "            model=qa_model,\n",
    "            tokenizer=qa_tokenizer,\n",
    "            device=-1\n",
    "        )\n",
    "        \n",
    "        correct = 0\n",
    "        for item in tqdm(squad_sample, desc=\"SQuAD Evaluation\"):\n",
    "            try:\n",
    "                result = qa_pipeline(\n",
    "                    question=item['question'],\n",
    "                    context=item['context'],\n",
    "                    handle_impossible_answer=True\n",
    "                )\n",
    "                if any(result['answer'].strip().lower() in ans.lower() for ans in item['answers']['text']):\n",
    "                    correct += 1\n",
    "            except Exception as e:\n",
    "                continue\n",
    "        results['squad_accuracy'] = correct / len(squad_sample) if squad_sample else 0\n",
    "\n",
    "        # GLUE Evaluation (SST-2)\n",
    "        try:\n",
    "            dataset = load_dataset('glue', 'sst2', split='validation').select(range(10))\n",
    "            encoded = tokenizer(\n",
    "                dataset['sentence'],\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                return_tensors='pt',\n",
    "                return_token_type_ids=True\n",
    "            )\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(\n",
    "                    input_ids=encoded['input_ids'],\n",
    "                    attention_mask=encoded['attention_mask'],\n",
    "                    token_type_ids=encoded.get('token_type_ids', None)\n",
    "                )\n",
    "                preds = outputs.logits.argmax(-1)\n",
    "                accuracy = (preds == torch.tensor(dataset['label'])).float().mean()\n",
    "                results['sst2_accuracy'] = accuracy.item()\n",
    "        except Exception as e:\n",
    "            print(f\"SST-2 Error: {str(e)}\")\n",
    "            results['sst2_error'] = str(e)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Language Skills Error: {str(e)}\")\n",
    "        results['language_skills_error'] = str(e)\n",
    "        \n",
    "    return results\n",
    "\n",
    "def evaluate_math_problem_solving(model, tokenizer):\n",
    "    print(\"Evaluating math problem-solving...\")\n",
    "    results = {}\n",
    "    try:\n",
    "        math_sample = load_dataset('math_dataset', 'arithmetic__add_or_sub', split='train', trust_remote_code=True).select(range(10))\n",
    "        correct = 0\n",
    "        for item in tqdm(math_sample, desc=\"Math Evaluation\"):\n",
    "            inputs = tokenizer(\n",
    "                f\"Problem: {item['question']} Answer:\",\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True\n",
    "            )\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "                predicted = outputs.logits.argmax(-1).item()\n",
    "                try:\n",
    "                    correct += int(str(predicted) == str(item['answer']))\n",
    "                except:\n",
    "                    continue\n",
    "        results['math_accuracy'] = correct / len(math_sample) if math_sample else 0\n",
    "    except Exception as e:\n",
    "        print(f\"Math Error: {str(e)}\")\n",
    "        results['math_error'] = str(e)\n",
    "    return results\n",
    "\n",
    "def evaluate_coding(model, tokenizer):\n",
    "    print(\"Evaluating coding skills...\")\n",
    "    results = {}\n",
    "    try:\n",
    "        code_sample = load_dataset('code_search_net', 'python', split='validation', trust_remote_code=True).select(range(10))\n",
    "        correct = 0\n",
    "        for item in tqdm(code_sample, desc=\"Coding Evaluation\"):\n",
    "            inputs = tokenizer(\n",
    "                item['whole_func_string'],\n",
    "                truncation=True,\n",
    "                padding=True,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "                predictions = outputs.logits.argmax(-1)\n",
    "                try:\n",
    "                    decoded = tokenizer.decode(predictions[0], skip_special_tokens=True)\n",
    "                    correct += int(decoded in item['whole_func_string'])\n",
    "                except:\n",
    "                    continue\n",
    "        results['code_accuracy'] = correct / len(code_sample) if code_sample else 0\n",
    "    except Exception as e:\n",
    "        print(f\"Coding Error: {str(e)}\")\n",
    "        results['code_error'] = str(e)\n",
    "    return results\n",
    "\n",
    "def evaluate_conversation_quality(model, tokenizer):\n",
    "    print(\"Evaluating conversation quality...\")\n",
    "    results = {}\n",
    "    try:\n",
    "        dialog_sample = load_dataset('daily_dialog', split='validation').select(range(10))\n",
    "        scores = []\n",
    "        for item in tqdm(dialog_sample, desc=\"Conversation Evaluation\"):\n",
    "            context = \" \".join(item['dialog'][:-1])\n",
    "            response = item['dialog'][-1]\n",
    "            inputs = tokenizer(\n",
    "                context,\n",
    "                response,\n",
    "                truncation=True,\n",
    "                padding=True,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "                scores.append(torch.softmax(outputs.logits, dim=-1)[0][1].item())\n",
    "        results['conversation_score'] = np.mean(scores) if scores else 0\n",
    "    except Exception as e:\n",
    "        print(f\"Conversation Error: {str(e)}\")\n",
    "        results['conversation_error'] = str(e)\n",
    "    return results\n",
    "\n",
    "def evaluate_model_performance(model, tokenizer):\n",
    "    results = {}\n",
    "    results.update(evaluate_language_skills(model, tokenizer))\n",
    "    # results.update(evaluate_math_problem_solving(model, tokenizer))\n",
    "    results.update(evaluate_coding(model, tokenizer))\n",
    "    results.update(evaluate_conversation_quality(model, tokenizer))\n",
    "    print(\"\\nFinal Evaluation Results:\")\n",
    "    for k, v in results.items():\n",
    "        print(f\"{k}: {v:.4f}\" if isinstance(v, float) else f\"{k}: {v}\")\n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = SimpleTransformerConfig(num_labels=2)\n",
    "    model = SimpleTransformerForSequenceClassification(config)\n",
    "    tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "    \n",
    "    evaluation_results = evaluate_model_performance(model, tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
