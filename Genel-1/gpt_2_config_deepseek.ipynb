{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "from flax import struct\n",
    "import optax\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 1. Flash Attention + Sparse MoE\n",
    "class FlashMoeAttention(nn.Module):\n",
    "    num_heads: int\n",
    "    num_experts: int = 8\n",
    "    top_k: int = 2\n",
    "    dtype: jnp.dtype = jnp.bfloat16\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        # Flash Attention\n",
    "        batch, seq_len, dim = x.shape\n",
    "        qkv = nn.Dense(dim*3, dtype=self.dtype)(x).reshape(batch, seq_len, 3, self.num_heads, dim//self.num_heads)\n",
    "        q, k, v = jnp.split(qkv, 3, axis=2)\n",
    "        attn_weights = jax.scipy.special.logsumexp(\n",
    "            jnp.einsum('bqhd,bkhd->bhqk', q, k) / jnp.sqrt(dim), axis=-1\n",
    "        )\n",
    "        attn_output = jnp.einsum('bhqk,bkhd->bqhd', nn.softmax(attn_weights), v)\n",
    "        attn_output = attn_output.reshape(batch, seq_len, dim)\n",
    "\n",
    "        # MoE (Mixture of Experts)\n",
    "        gate = nn.Dense(self.num_experts, dtype=self.dtype)(x)\n",
    "        gate = jax.nn.softmax(gate, axis=-1)\n",
    "        top_k_gates, top_k_indices = jax.lax.top_k(gate, self.top_k)\n",
    "        \n",
    "        expert_outputs = []\n",
    "        for i in range(self.num_experts):\n",
    "            expert = nn.Dense(dim, dtype=self.dtype)(x)\n",
    "            mask = (top_k_indices == i).astype(jnp.float32)\n",
    "            expert_outputs.append(expert * mask[..., None] * top_k_gates[..., None])\n",
    "        \n",
    "        return attn_output + sum(expert_outputs)\n",
    "\n",
    "# 2. Ultra Derin Dil Modeli\n",
    "class DeepSeekClone(nn.Module):\n",
    "    vocab_size: int\n",
    "    num_layers: int = 32\n",
    "    num_heads: int = 16\n",
    "    dim: int = 2048\n",
    "    expert_count: int = 8\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, inputs):\n",
    "        x = nn.Embed(self.vocab_size, self.dim)(inputs)\n",
    "        for _ in range(self.num_layers):\n",
    "            # Pre-LayerNorm\n",
    "            x = nn.LayerNorm()(x)\n",
    "            \n",
    "            # Flash+Moe Attention\n",
    "            residual = x\n",
    "            x = FlashMoeAttention(num_heads=self.num_heads)(x)\n",
    "            x = residual + x\n",
    "            \n",
    "            # Gated FFN\n",
    "            x = nn.LayerNorm()(x)\n",
    "            x = nn.Dense(self.dim*4)(x)\n",
    "            x = nn.gelu(x)\n",
    "            x = nn.Dense(self.dim)(x)\n",
    "        \n",
    "        return nn.Dense(self.vocab_size)(x)\n",
    "\n",
    "# 3. Optimizasyon ve EÄŸitim State\n",
    "def create_train_state(rng, config):\n",
    "    model = DeepSeekClone(**config)\n",
    "    params = model.init(rng, jnp.ones((1, 512), dtype=jnp.int32))['params']\n",
    "    tx = optax.chain(\n",
    "        optax.clip_by_global_norm(1.0),\n",
    "        optax.adamw(learning_rate=3e-5, b1=0.9, b2=0.98),\n",
    "        optax.add_decayed_weights(0.1)\n",
    "    )\n",
    "    return train_state.TrainState.create(\n",
    "        apply_fn=model.apply, params=params, tx=tx\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Streaming Data Pipeline (TF + JAX)\n",
    "def build_data_pipeline(batch_size=256, seq_len=512):\n",
    "    ds = load_dataset(\"c4\", \"en\", split=\"train\", streaming=True)\n",
    "    ds = ds.shuffle(buffer_size=10000).take(1_000_000)\n",
    "    \n",
    "    vectorizer = TextVectorization(\n",
    "        output_sequence_length=seq_len,\n",
    "        standardize=\"lower_and_strip_punctuation\"\n",
    "    )\n",
    "    vectorizer.adapt(ds.map(lambda x: x[\"text\"]))\n",
    "    \n",
    "    def encode_fn(text):\n",
    "        tokens = vectorizer(text).numpy().astype(\"int32\")\n",
    "        return {\"input_ids\": tokens[:-1], \"labels\": tokens[1:]}\n",
    "    \n",
    "    return ds.map(encode_fn).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# 2. JAX Data Loader\n",
    "def jax_data_loader(ds):\n",
    "    for batch in ds.as_numpy_iterator():\n",
    "        yield jax.tree_map(jnp.asarray, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.pmap\n",
    "def train_step(state, batch):\n",
    "    def loss_fn(params):\n",
    "        logits = state.apply_fn({'params': params}, batch['input_ids'])\n",
    "        loss = optax.softmax_cross_entropy_with_integer_labels(\n",
    "            logits[..., :-1, :], batch['labels']\n",
    "        ).mean()\n",
    "        return loss\n",
    "    \n",
    "    grad_fn = jax.value_and_grad(loss_fn)\n",
    "    loss, grads = grad_fn(state.params)\n",
    "    grads = jax.lax.pmean(grads, \"batch\")\n",
    "    new_state = state.apply_gradients(grads=grads)\n",
    "    return new_state, loss\n",
    "\n",
    "def train():\n",
    "    # TPU/GPU Setup\n",
    "    devices = jax.local_devices()\n",
    "    print(f\"Using {len(devices)} devices: {devices}\")\n",
    "    \n",
    "    # Config\n",
    "    config = {\n",
    "        \"vocab_size\": 50000,\n",
    "        \"num_layers\": 24,\n",
    "        \"dim\": 4096,\n",
    "        \"num_heads\": 32\n",
    "    }\n",
    "    \n",
    "    # Init State\n",
    "    rng = jax.random.PRNGKey(0)\n",
    "    state = create_train_state(rng, config)\n",
    "    state = jax.device_put_replicated(state, devices)\n",
    "    \n",
    "    # Data\n",
    "    ds = build_data_pipeline()\n",
    "    loader = jax_data_loader(ds)\n",
    "    \n",
    "    # Training\n",
    "    for epoch in range(10):\n",
    "        total_loss = 0.0\n",
    "        for step, batch in enumerate(loader):\n",
    "            batch = jax.tree_map(lambda x: x.reshape(len(devices), -1, *x.shape[1:]), batch)\n",
    "            state, loss = train_step(state, batch)\n",
    "            total_loss += loss.mean().item()\n",
    "            \n",
    "            if step % 100 == 0:\n",
    "                print(f\"Step {step} | Loss: {loss.mean().item():.4f}\")\n",
    "        \n",
    "        print(f\"Epoch {epoch} | Avg Loss: {total_loss/(step+1):.4f}\")\n",
    "        # Model Checkpoint\n",
    "        jax.checkpoint.save(f\"model_epoch_{epoch}\", state.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Blockwise Quantization (8-bit training)\n",
    "def quantize(params):\n",
    "    return jax.tree_map(\n",
    "        lambda x: jnp.round(x * 127 / jnp.max(jnp.abs(x))).astype(jnp.int8),\n",
    "        params\n",
    "    )\n",
    "\n",
    "# 2. Gradient Checkpointing\n",
    "@jax.checkpoint\n",
    "def memory_efficient_forward(params, inputs):\n",
    "    return model.apply({'params': params}, inputs)\n",
    "\n",
    "# 3. Dynamic Sequence Packing\n",
    "def pack_sequences(batch, max_length=4096):\n",
    "    packed = []\n",
    "    current = []\n",
    "    for seq in batch:\n",
    "        if len(current) + len(seq) > max_length:\n",
    "            packed.append(jnp.array(current))\n",
    "            current = []\n",
    "        current.extend(seq)\n",
    "    return jnp.stack(packed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
